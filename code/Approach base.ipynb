{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os.path\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Bidirectional, Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import SpatialDropout1D, Concatenate, Maximum, Activation, Reshape, Flatten\n",
    "from keras.layers import RepeatVector, Permute, Merge, TimeDistributed, Multiply, Lambda, Concatenate\n",
    "from keras import backend as K\n",
    "\n",
    "max_features = 100\n",
    "maxlen = 80\n",
    "embedding_size = 32\n",
    "lstm_output_size = 32\n",
    "batch_size = 128\n",
    "epochs_polarity = 50\n",
    "epochs_irony= 10\n",
    "\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos ironía\n",
    "\n",
    "Se cargan los datos de ironía, por cada tarea:\n",
    "\n",
    "* taskA: Clasificador binario\n",
    "* taskB: Clasificador multi-clase (4?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening files\n",
      "../SemEval2018-Task3/infotec_train_dev/SemEval2018-T3-taskA_train.json\n",
      "Size 2683\n",
      "Opening files\n",
      "../SemEval2018-Task3/infotec_train_dev/SemEval2018-T3-taskA_test.json\n",
      "Size 1151\n"
     ]
    }
   ],
   "source": [
    "def load_files(files):\n",
    "    json_files=[]\n",
    "    print(\"Opening files\")\n",
    "    for filename in files:\n",
    "        print(filename)\n",
    "        data=[]\n",
    "        for line in codecs.open(filename):\n",
    "            data.append(json.loads(line))\n",
    "        json_files.append(data)\n",
    "        print(\"Size\",len(json_files[-1]))\n",
    "    return json_files\n",
    "\n",
    "\n",
    "tasks=[\"taskA\"]\n",
    "#tasks=[\"taskA\",\"taskB\"]\n",
    "dirname=\"../SemEval2018-Task3/infotec_train_dev\"\n",
    "basename=\"SemEval2018-T3-{0}_{1}.json\"\n",
    "train_files=[os.path.join(dirname,basename.format(task,'train')) for task in tasks]\n",
    "test_files=[os.path.join(dirname,basename.format(task,'test')) for task in tasks]\n",
    "\n",
    "train_json=load_files(train_files)\n",
    "test_json=load_files(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos de polaridad\n",
    "\n",
    "Cargando datos de polaridad, una sola tarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening files\n",
      "../extras/En.json\n",
      "Size 45748\n"
     ]
    }
   ],
   "source": [
    "train_polarity_json=load_files([\"../extras/En.json\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 2683\n",
      "Size: 1151\n",
      "Size: 45748\n"
     ]
    }
   ],
   "source": [
    "# Configurando datos de ironía\n",
    "\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "nltk_tok=TweetTokenizer(reduce_len=True)\n",
    "\n",
    "data_train=[]\n",
    "data_test=[]\n",
    "for i,t in enumerate(tasks):\n",
    "    text_train=[\" \".join(nltk_tok.tokenize(j['text'])) for j in train_json[i]]\n",
    "    class_train=[j['klass'] for j in train_json[i]]\n",
    "    data_train.append(list(zip(text_train,class_train)))\n",
    "    text_test=[\" \".join(nltk_tok.tokenize(j['text'])) for j in test_json[i]]\n",
    "    class_test=[j['klass'] for j in test_json[i]]\n",
    "    data_test.append(list(zip(text_test,class_test)))\n",
    "\n",
    "\n",
    "for i,t in enumerate(data_train):\n",
    "    print(\"Size:\",len(data_train[i]))\n",
    "    print(\"Size:\",len(data_test[i]))\n",
    "    \n",
    "# Configurando datos de polaridad\n",
    "\n",
    "def pol2class(k):\n",
    "    onehot=[[1,0,0],[0,1,0],[0,0,1]]\n",
    "    return onehot[['neutral','positive','negative'].index(k)]\n",
    "\n",
    "text_polarity_train=[\" \".join(nltk_tok.tokenize(j['text'])) for j in train_polarity_json]\n",
    "class_polarity_train=[pol2class(j['klass']) for j in train_polarity_json]\n",
    "data_polarity_train=list(zip(text_polarity_train,class_polarity_train))\n",
    "\n",
    "print(\"Size:\",len(data_polarity_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "  \n",
    "\n",
    "def normalizeText(tweet):\n",
    "    #tweet = re.sub(r'#(S|s)arcasm|#(I|i)rony','',tweet)\n",
    "    #tweet = re.sub(r'#SARCASM|#IRONY','',tweet)\n",
    "    tweet = re.sub(r'https?://t\\.co/.(\\w|\\d)+','http', tweet) #tweet link\n",
    "    tweet = re.sub(r'fb\\.me/.(\\w|\\d)+','fb', tweet) #tweet link\n",
    "    tweet = re.sub(r'https?://.+','http', tweet) #tweet link\n",
    "    tweet = re.sub(r'@.\\w*','@',tweet)\n",
    "    #tweet = re.sub(r'#','',tweet)\n",
    "    return tweet\n",
    "\n",
    "def buildTokenizer(tweets):\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_features,lower=False, filters='\\t\\n', split=\" \",char_level=True )\n",
    "    tokenizer.fit_on_texts([\" \".join(nltk_tok.tokenize(t)) for t in tweets])\n",
    "    return tokenizer\n",
    "\n",
    "def text2seq(tok,tweet):\n",
    "    return tok.texts_to_sequences([normalizeText(tweet)])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_counts 135\n",
      "word_docs 135\n",
      "word_index 135\n",
      "Top words\n",
      "   (1009425) , e (392998) , t (341380) , a (309908) , o (302099) , n (255361) , i (244677) , r (214410) , s (211597) , h (183558) , l (147455) , d (129392) , u (107501) , y (100262) , m (99122) , g (84966) , c (83140) , w (75137) , . (68693) , p (68296) , f (62750) , b (58628) , k (42424) , v (34797) , S (30490) , I (29341) , ' (25133) , T (24650) , , (23257) , A (20543) , M (20280) , C (19217) , @ (19209) , B (17370) , ! (16269) , D (14741) , # (14525) , W (14130) , 1 (14067) , N (13406) , P (13032) , F (12931) , R (12859) , H (12329) , O (11890) , G (11358) , L (11191) , E (10907) , 2 (10893) , : (10709) \n",
      "Last words\n",
      " > (311) , < (292) , % (272) , = (258) , ] (207) , [ (206) , ~ (200) , ^ (142) , ` (35) , \\ (33) , ️ (25) , } (9) , – (8) , { (7) , £ (5) , \n",
      " (5) , ’ (3) , ☆ (3) , ・ (3) , — (2) , Ｏ (2) , Ｌ (2) , İ (2) , ℃ (2) , 你 (2) , ï (1) , 󾍁 (1) , ° (1) , ง (1) , ว (1) , ย (1) , Ｆ (1) , Ｗ (1) , « (1) , ó (1) , í (1) , ⁰ (1) , Ã (1) , ¢ (1) , Ë (1) , œ (1) , Å (1) , ö (1) , 就 (1) , 算 (1) , 隔 (1) , 格 (1) , 我 (1) , 都 (1) , 知 (1) \n"
     ]
    }
   ],
   "source": [
    "toks=[]\n",
    "maps_=[]\n",
    "for i,t in enumerate(tasks):\n",
    "    tweets=[t for t,c in data_train[i]]\n",
    "    tweets_=[t for t,c in data_polarity_train]\n",
    "    toks.append(buildTokenizer([normalizeText(t) for t in tweets+tweets_]))\n",
    "    maps_.append({v: k for k, v in toks[-1].word_index.items()})\n",
    "    print(\"word_counts\",len(toks[-1].word_counts))\n",
    "    print(\"word_docs\",len(toks[-1].word_docs))\n",
    "    print(\"word_index\",len(toks[-1].word_index))\n",
    "    print(\"Top words\\n\",\", \".join([\"{0} ({1}) \".format(maps_[-1][f+1],toks[i].word_counts[maps_[-1][f+1]]) for f in range(50)]))\n",
    "    print(\"Last words\\n\",\", \".join([\"{0} ({1}) \".format(maps_[-1][f],toks[i].word_counts[maps_[-1][f]]) for f in range(len(maps_[i])-50,len(maps_[i]))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example train in taskA\n",
      "The mouse's first incepted memory was just the sound : BRAAAWWWP !\n",
      "T h e   m o u s e ' s   f i r s t   i n c e p t e d   m e m o r y   w a s   j u s t   t h e   s o u n d   :   B R A A A W W W P   !\n",
      ">>I LOVE not sleeping . It's the best .\n",
      "I   L O V E   n o t   s l e e p i n g   .   I t ' s   t h e   b e s t   .\n",
      ">>Religion is unfounded , else , Allah would have saved the kids . . @tariqmushtaqkh @nicpradhan #PeshawarAttack #PakSchoolSiege\n",
      "R e l i g i o n   i s   u n f o u n d e d   ,   e l s e   ,   A l l a h   w o u l d   h a v e   s a v e d   t h e   k i d s   .   .   @   @   # P e s h a w a r A t t a c k   # P a k S c h o o l S i e g e\n",
      ">>Love how I came into work at 8 because Charlie said we were busy ... 3 people in 45 minutes , yeah we got this place packed Charlie .\n",
      "L o v e   h o w   I   c a m e   i n t o   w o r k   a t   8   b e c a u s e   C h a r l i e   s a i d   w e   w e r e   b u s y   . . .   3   p e o p l e   i n   4 5   m i n u t e s   ,   y e a h   w e   g o t   t h i s   p l a c e   p a c k e d   C h a r l i e   .\n",
      ">>Thx for catching on #urock\n",
      "T h x   f o r   c a t c h i n g   o n   # u r o c k\n",
      ">>@DFDSUKUpdates why all the delays ? happy\n",
      "@   w h y   a l l   t h e   d e l a y s   ?   h a p p y\n",
      ">>I am extremely excited to know what Reza Aslan and CJ Werleman think about what happened in Pakistan #PeshawarAttack\n",
      "I   a m   e x t r e m e l y   e x c i t e d   t o   k n o w   w h a t   R e z a   A s l a n   a n d   C J   W e r l e m a n   t h i n k   a b o u t   w h a t   h a p p e n e d   i n   P a k i s t a n   # P e s h a w a r A t t a c k\n",
      ">>Kebabs ordered .. | Snug on sofa watching shit tele with dad : two_hearts :: purple_heart :\n",
      "K e b a b s   o r d e r e d   . .   |   S n u g   o n   s o f a   w a t c h i n g   s h i t   t e l e   w i t h   d a d   :   t w o _ h e a r t s   : :   p u r p l e _ h e a r t   :\n",
      ">>Still feeling sorry for myself about being to ill to go out last night ! ! ! : loudly_crying_face :: crying_face : here's my memories from ... http://t.co/I3fIk1k6ek\n",
      "S t i l l   f e e l i n g   s o r r y   f o r   m y s e l f   a b o u t   b e i n g   t o   i l l   t o   g o   o u t   l a s t   n i g h t   !   !   !   :   l o u d l y _ c r y i n g _ f a c e   : :   c r y i n g _ f a c e   :   h e r e ' s   m y   m e m o r i e s   f r o m   . . .   h t t p\n",
      ">>@quiksilverindia The only place where you don't wanna sit on the seat that you earned . | #Dancing | #QuiksilverGoesSupersonic\n",
      "@   T h e   o n l y   p l a c e   w h e r e   y o u   d o n ' t   w a n n a   s i t   o n   t h e   s e a t   t h a t   y o u   e a r n e d   .   |   # D a n c i n g   |   # Q u i k s i l v e r G o e s S u p e r s o n i c\n",
      ">>Accidentally breaking your own computer when your tech support ! Haha , talk about employee of the month #techSupport :p ersonal_computer :: floppy_disk :: pistol :\n",
      "A c c i d e n t a l l y   b r e a k i n g   y o u r   o w n   c o m p u t e r   w h e n   y o u r   t e c h   s u p p o r t   !   H a h a   ,   t a l k   a b o u t   e m p l o y e e   o f   t h e   m o n t h   # t e c h S u p p o r t   : p   e r s o n a l _ c o m p u t e r   : :   f l o p p y _ d i s k   : :   p i s t o l   :\n",
      ">>BITCH WHERE THE FUCK U THINK IM GOING ? ? ? | BETTER SIT ON DOWN WITH THE REST OF THESE BITCHES WAITTNG FOR ME TO MOVE ... NEVA\n",
      "B I T C H   W H E R E   T H E   F U C K   U   T H I N K   I M   G O I N G   ?   ?   ?   |   B E T T E R   S I T   O N   D O W N   W I T H   T H E   R E S T   O F   T H E S E   B I T C H E S   W A I T T N G   F O R   M E   T O   M O V E   . . .   N E V A\n",
      ">>Hey everyone ... I want this as a Christmas gift :) Can you send me one ? @NVIDIAGeForce #GTX980 http://t.co/nc7S4QOqpW\n",
      "H e y   e v e r y o n e   . . .   I   w a n t   t h i s   a s   a   C h r i s t m a s   g i f t   : )   C a n   y o u   s e n d   m e   o n e   ?   @   # G T X 9 8 0   h t t p\n",
      ">>2015 prediction : Putin ditches the Ruble and adopts Bitcoin\n",
      "2 0 1 5   p r e d i c t i o n   :   P u t i n   d i t c h e s   t h e   R u b l e   a n d   a d o p t s   B i t c o i n\n",
      ">>#nuffsaid #stupidity #hadenough lols : face_with_tears_of_joy :: smiling_face_with_open_mouth : http://t.co/0iSZ1ExuLn\n",
      "# n u f f s a i d   # s t u p i d i t y   # h a d e n o u g h   l o l s   :   f a c e _ w i t h _ t e a r s _ o f _ j o y   : :   s m i l i n g _ f a c e _ w i t h _ o p e n _ m o u t h   :   h t t p\n",
      ">>alot-still not sussed out how yo keep in touch with peop ! e via twitterTwitterversary\n",
      "a l o t - s t i l l   n o t   s u s s e d   o u t   h o w   y o   k e e p   i n   t o u c h   w i t h   p e o p   !   e   v i a   t w i t t e r T w i t t e r v e r s a r y\n",
      ">>Thanks mum for farting ... Not once , but twice during dinner with the BF . #NiceOne\n",
      "T h a n k s   m u m   f o r   f a r t i n g   . . .   N o t   o n c e   ,   b u t   t w i c e   d u r i n g   d i n n e r   w i t h   t h e   B F   .   # N i c e O n e\n",
      ">>Shoes on make-up done ! I'm ready\n",
      "S h o e s   o n   m a k e - u p   d o n e   !   I ' m   r e a d y\n",
      ">>@Bungie @DestinyTheGame thanks for plan c two weeks in a row ! Getting rid of exotic engrams is also an amazing move !\n",
      "@   @   t h a n k s   f o r   p l a n   c   t w o   w e e k s   i n   a   r o w   !   G e t t i n g   r i d   o f   e x o t i c   e n g r a m s   i s   a l s o   a n   a m a z i n g   m o v e   !\n",
      ">>@darrellr79 @EBJunkies I'd prefer the slew of #Ferguson protestors across the US ... they stop traffic shut dwn bridges\n",
      "@   @   I ' d   p r e f e r   t h e   s l e w   o f   # F e r g u s o n   p r o t e s t o r s   a c r o s s   t h e   U S   . . .   t h e y   s t o p   t r a f f i c   s h u t   d w n   b r i d g e s\n",
      ">>@LOLGOP Fox News criticizing poor journalism regarding the UVA incident is hilarious\n",
      "@   F o x   N e w s   c r i t i c i z i n g   p o o r   j o u r n a l i s m   r e g a r d i n g   t h e   U V A   i n c i d e n t   i s   h i l a r i o u s\n",
      ">>I just love a chaotic school run ! ! ! ...\n",
      "I   j u s t   l o v e   a   c h a o t i c   s c h o o l   r u n   !   !   !   . . .\n",
      ">>I'd rather you know and be upset than hide it for you to only question things later\n",
      "I ' d   r a t h e r   y o u   k n o w   a n d   b e   u p s e t   t h a n   h i d e   i t   f o r   y o u   t o   o n l y   q u e s t i o n   t h i n g s   l a t e r\n",
      ">>@TaylorLynn0022 | Ew you really did it ?\n",
      "@   |   E w   y o u   r e a l l y   d i d   i t   ?\n",
      ">>Left my lunch at home . Swansea canteen outdoing themselves with this generous and well priced portion of food http://t.co/jn7Kff2K18\n",
      "L e f t   m y   l u n c h   a t   h o m e   .   S w a n s e a   c a n t e e n   o u t d o i n g   t h e m s e l v e s   w i t h   t h i s   g e n e r o u s   a n d   w e l l   p r i c e d   p o r t i o n   o f   f o o d   h t t p\n",
      ">>@haleymae21 Ha ! I don't have haters . Just a few misguided souls who thought my mentions were a bulletin board for their issues . ;)\n",
      "@   H a   !   I   d o n ' t   h a v e   h a t e r s   .   J u s t   a   f e w   m i s g u i d e d   s o u l s   w h o   t h o u g h t   m y   m e n t i o n s   w e r e   a   b u l l e t i n   b o a r d   f o r   t h e i r   i s s u e s   .   ; )\n",
      ">>@CarlosDenWA great Christmas present . Unreal what the alcohol industry will do to lure young people . #sad #nosocialconscience\n",
      "@   g r e a t   C h r i s t m a s   p r e s e n t   .   U n r e a l   w h a t   t h e   a l c o h o l   i n d u s t r y   w i l l   d o   t o   l u r e   y o u n g   p e o p l e   .   # s a d   # n o s o c i a l c o n s c i e n c e\n",
      ">>@jcpetit4 dusty as usual : electric_plug : #redbucket http://t.co/U8llvttn59\n",
      "@   d u s t y   a s   u s u a l   :   e l e c t r i c _ p l u g   :   # r e d b u c k e t   h t t p\n",
      ">>@Sahelanth @QuerierNew @hudds1 because as long as she's a victim she'll have white knights trying to save that damsel in distress .\n",
      "@   @   @   b e c a u s e   a s   l o n g   a s   s h e ' s   a   v i c t i m   s h e ' l l   h a v e   w h i t e   k n i g h t s   t r y i n g   t o   s a v e   t h a t   d a m s e l   i n   d i s t r e s s   .\n",
      ">>@steigerwaldino Nah , it's better we all act like the North Korean govt and police people's private thoughts .\n",
      "@   N a h   ,   i t ' s   b e t t e r   w e   a l l   a c t   l i k e   t h e   N o r t h   K o r e a n   g o v t   a n d   p o l i c e   p e o p l e ' s   p r i v a t e   t h o u g h t s   .\n",
      ">>A voice of reason crying in the wilderness of hysteria . http://t.co/GHWycSm5bW via @sharethis\n",
      "A   v o i c e   o f   r e a s o n   c r y i n g   i n   t h e   w i l d e r n e s s   o f   h y s t e r i a   .   h t t p   v i a   @\n",
      ">>Towing company commuter car being towed . http://t.co/O3NbsLQahp\n",
      "T o w i n g   c o m p a n y   c o m m u t e r   c a r   b e i n g   t o w e d   .   h t t p\n",
      ">>@SouthamptonFC @LFC @SkySports Again fans come second . Thanks again\n",
      "@   @   @   A g a i n   f a n s   c o m e   s e c o n d   .   T h a n k s   a g a i n\n",
      ">>Seems as if @ProudMaryBoise wants to endorse me on LinkedIn for - any thoughts on this from the #OMCchat crowd ?\n",
      "S e e m s   a s   i f   @   w a n t s   t o   e n d o r s e   m e   o n   L i n k e d I n   f o r   -   a n y   t h o u g h t s   o n   t h i s   f r o m   t h e   # O M C c h a t   c r o w d   ?\n",
      ">>Thank you @angiegalifas for these amazing candles from the new series of @victoriassecret #atmosphere ... http://t.co/JOhwVxVH0b\n",
      "T h a n k   y o u   @   f o r   t h e s e   a m a z i n g   c a n d l e s   f r o m   t h e   n e w   s e r i e s   o f   @   # a t m o s p h e r e   . . .   h t t p\n",
      ">>father sent me down to buy ingredients cos he wanna fry some noodles up i remembered everything else but the noodles #somuchwin\n",
      "f a t h e r   s e n t   m e   d o w n   t o   b u y   i n g r e d i e n t s   c o s   h e   w a n n a   f r y   s o m e   n o o d l e s   u p   i   r e m e m b e r e d   e v e r y t h i n g   e l s e   b u t   t h e   n o o d l e s   # s o m u c h w i n\n",
      ">>@IkeMagnifico Funny thing is now she's a security guard at Fed Ex in LA where I pick up all the time .\n",
      "@   F u n n y   t h i n g   i s   n o w   s h e ' s   a   s e c u r i t y   g u a r d   a t   F e d   E x   i n   L A   w h e r e   I   p i c k   u p   a l l   t h e   t i m e   .\n",
      ">>Packers fans on Twitter are on full meltdown mode ... R . E . L . A . X .\n",
      "P a c k e r s   f a n s   o n   T w i t t e r   a r e   o n   f u l l   m e l t d o w n   m o d e   . . .   R   .   E   .   L   .   A   .   X   .\n",
      ">>@RandBall gotta hand it to them , really . : clapping_hands_sign :: clapping_hands_sign :: clapping_hands_sign :\n",
      "@   g o t t a   h a n d   i t   t o   t h e m   ,   r e a l l y   .   :   c l a p p i n g _ h a n d s _ s i g n   : :   c l a p p i n g _ h a n d s _ s i g n   : :   c l a p p i n g _ h a n d s _ s i g n   :\n",
      ">>@KevinnCyrus lol of course it's the best ! !\n",
      "@   l o l   o f   c o u r s e   i t ' s   t h e   b e s t   !   !\n",
      ">>@KevinStryker What do you know about #iubb ? Tom Crean is a coaching genius !\n",
      "@   W h a t   d o   y o u   k n o w   a b o u t   # i u b b   ?   T o m   C r e a n   i s   a   c o a c h i n g   g e n i u s   !\n",
      ">>@Moose_eBooks You seriously bought a giant TV to play in the box ?\n",
      "@   Y o u   s e r i o u s l y   b o u g h t   a   g i a n t   T V   t o   p l a y   i n   t h e   b o x   ?\n",
      ">>the people #BillCosby looked down upon are ones coming to his defense . This isn't about race , it's about rape ! http://t.co/PD4GUCPPKV\n",
      "t h e   p e o p l e   # B i l l C o s b y   l o o k e d   d o w n   u p o n   a r e   o n e s   c o m i n g   t o   h i s   d e f e n s e   .   T h i s   i s n ' t   a b o u t   r a c e   ,   i t ' s   a b o u t   r a p e   !   h t t p\n",
      ">>they don't sing live , but they sure are hella good looking #smh\n",
      "t h e y   d o n ' t   s i n g   l i v e   ,   b u t   t h e y   s u r e   a r e   h e l l a   g o o d   l o o k i n g   # s m h\n",
      ">>I have such a loving family\n",
      "I   h a v e   s u c h   a   l o v i n g   f a m i l y\n",
      ">>@Monaiza_Diva kabhi I'm messier than usual messy wesay :p ersevering_face : I need chai & I'm resting for last 2 days . May be this is the reason .\n",
      "@   k a b h i   I ' m   m e s s i e r   t h a n   u s u a l   m e s s y   w e s a y   : p   e r s e v e r i n g _ f a c e   :   I   n e e d   c h a i   &   I ' m   r e s t i n g   f o r   l a s t   2   d a y s   .   M a y   b e   t h i s   i s   t h e   r e a s o n   .\n",
      ">>@chriscomben @lexpersaud it's in my Priory contract to wear those . Hugely popular with Priory Ultras #does #count #xxx\n",
      "@   @   i t ' s   i n   m y   P r i o r y   c o n t r a c t   t o   w e a r   t h o s e   .   H u g e l y   p o p u l a r   w i t h   P r i o r y   U l t r a s   # d o e s   # c o u n t   # x x x\n",
      ">>I just love 160 question tests in the morning .\n",
      "I   j u s t   l o v e   1 6 0   q u e s t i o n   t e s t s   i n   t h e   m o r n i n g   .\n",
      ">>Si Ms . Educ hehehe clear my ignorance : smiling_face_with_heart-shaped_eyes :: face_with_tears_of_joy :\n",
      "S i   M s   .   E d u c   h e h e h e   c l e a r   m y   i g n o r a n c e   :   s m i l i n g _ f a c e _ w i t h _ h e a r t - s h a p e d _ e y e s   : :   f a c e _ w i t h _ t e a r s _ o f _ j o y   :\n",
      ">>http://t.co/RfwyHOMH2S #fOLLOW THE #MONEY #HOMEbiz NOT #mlm * * * NOTE YOU CAN GET PAID FOR POSTING ON ... http://t.co/Dkr1MjdsTd\n",
      "h t t p   # f O L L O W   T H E   # M O N E Y   # H O M E b i z   N O T   # m l m   *   *   *   N O T E   Y O U   C A N   G E T   P A I D   F O R   P O S T I N G   O N   . . .   h t t p\n",
      "\n",
      "[0, 1, 1, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ir2class(k):\n",
    "    onehot=[[1,0,0,0,0],[0,1,0,0,0],[0,0,1,0,0],[0,0,0,1,0],[0,0,0,0,1]]\n",
    "    return onehot[['0','1','2',\"3\",\"4\"].index(k)]\n",
    "\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "X_test=[]\n",
    "y_test=[]\n",
    "for i,t in enumerate(tasks):\n",
    "    X_train.append([text2seq(toks[i],t) for t,c in data_train[i]])\n",
    "    if t==\"taskA\":\n",
    "        y_train.append([int(c) for t,c in data_train[i]])\n",
    "    else:\n",
    "        y_train.append([ir2class(c) for t,c in data_train[i]])\n",
    "    X_test.append([text2seq(toks[i],t) for t,c in data_test[i]])\n",
    "    if t==\"taskA\":\n",
    "        y_test.append([int(c) for t,c in data_test[i]])\n",
    "    else:\n",
    "        y_test.append([ir2class(c) for t,c in data_test[i]])\n",
    "        \n",
    "\n",
    "    print(\"Example train in\",t)\n",
    "    #print(\"\\n\".join([\" \".join([str(w) for w in S]) for S in X_train[i][:5]]))\n",
    "    original=[t for t,c in data_train[i][:50]]\n",
    "    tokenized=[\" \".join([maps_[i][w] for w in S if w]) for S in X_train[i][:50]]\n",
    "        \n",
    "    print(\"\\n>>\".join([\"{0}\\n{1}\".format(a,b) for a,b in zip(original,tokenized)]))\n",
    "    print()\n",
    "    print([c for c in y_train[i][:5]])\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example train\n",
      "F i o r i n a   b l a s t s   C l i n t o n   ,   a s k s   w h y   T r u m p   i s   M I A   -   R e p u b l i c a n   h o p e f u l   C a r l y   F i o r i n a   s a i d   M o n d a y   t h a t   s h e   f e e l s   l i k e   t   . . .   h t t p\n",
      "@   I ' m   d o w n   f o r   t h a t   o b v i o u s l y   ,   b u t   d o e s   N i n t e n d o   w a n t   t h a t   ?   T h e   w o r l d   m a y   n e v e r   k n o w\n",
      "@   T r u e   ,   r i g h t   n o w   t h e y   h a v e   t h e   b e s t   s q u a d   .   B u t   i f   M i l a n   c a n   p u l l   a   f e w   t r a n s f e r s   r i g h t   i n   J a n u a r y   ,   t h e r e ' s   h o p e   f o r   t o p   3   .\n",
      "# E M A   # M A S S I V E A C T I O N   t h i s   S u n d a y   a t   1 2   N o o n   i n   V e n i c e   B e a c h   a t   R o s e   A v e   &   O c e a n   F r o n t   W a l k   o n   t h e   s a n d   !   F r e e   . . .   h t t p\n",
      "f u l l   r e s u l t s   f r o m   E l d o r a   S p e e d w a y   !   f i n i s h e d   2 n d   w i t h   t h e   N R A   !   f b\n",
      "[[1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "X_polarity_train=[]\n",
    "y_polarity_train=[]\n",
    "for i,t in enumerate(tasks):\n",
    "    X_polarity_train.append([text2seq(toks[i],t) for t,c in data_polarity_train])\n",
    "    y_polarity_train.append([c for t,c in data_polarity_train])\n",
    "    \n",
    "print(\"Example train\")\n",
    "print(\"\\n\".join([\" \".join([maps_[i][w] for w in S if w]) for S in X_polarity_train[0][:5]]))\n",
    "print(y_polarity_train[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(task):\n",
    "    \n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    embeedings = Embedding(max_features, embedding_size)(inputs)\n",
    "    dropout=SpatialDropout1D(0.25)(embeedings)\n",
    "    \n",
    "    conv1 = Conv1D(16,\n",
    "                 3,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1)(dropout)\n",
    "    maxpool1 = MaxPooling1D(pool_size=pool_size)(conv1)\n",
    "    conv2 = Conv1D(16,\n",
    "                 5,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1)(dropout)\n",
    "    maxpool2 = MaxPooling1D(pool_size=pool_size)(conv2)\n",
    "    conv3= Conv1D(16,\n",
    "                 7,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1)(dropout)\n",
    "    maxpool3= MaxPooling1D(pool_size=pool_size)(conv3)\n",
    "    conv4= Conv1D(16,\n",
    "                 9,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1)(dropout)\n",
    "    maxpool4=MaxPooling1D(pool_size=pool_size)(conv4)\n",
    "    \n",
    "    concatenate = Concatenate(axis=1)([maxpool1,maxpool2,maxpool3,maxpool4])\n",
    "    bidirectional =  Bidirectional(LSTM(lstm_output_size, return_sequences=True))(concatenate)\n",
    "    \n",
    "    \n",
    "    a = Permute((2, 1))(bidirectional)\n",
    "    print(bidirectional.shape)\n",
    "    #a = Reshape((, maxlen*2))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(maxlen, activation='softmax')(a)\n",
    "    #if SINGLE_ATTENTION_VECTOR:\n",
    "    #    a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "    #    a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    attention_mul = Multiply()([bidirectional, a_probs])\n",
    "\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "    \n",
    "    \n",
    "    if task==\"taskA\":\n",
    "        irony=Dense(1, activation='sigmoid')(attention_mul)\n",
    "    if task==\"taskB\":\n",
    "        irony=Dense(5, activation='softmax')(attention_mul)\n",
    "    \n",
    "    polarity=Dense(3, activation='softmax')(attention_mul)\n",
    "    model_irony = Model(inputs=inputs, outputs=irony)\n",
    "    model_polarity = Model(inputs=inputs, outputs=polarity)\n",
    "    \n",
    "    #model.compile(loss='mse', optimizer='sgd')\n",
    "    return model_polarity, model_irony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model task taskA\n",
      "(?, ?, 64)\n"
     ]
    }
   ],
   "source": [
    "models=[]\n",
    "for i,t in enumerate(tasks):\n",
    "    print(\"Creating model task\",t)\n",
    "    models.append(build_model(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 80)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 80, 32)       3200        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 80, 32)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 80, 16)       1552        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 80, 16)       2576        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 80, 16)       3600        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 80, 16)       4624        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 20, 16)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 20, 16)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 20, 16)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 20, 16)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 80, 16)       0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 80, 64)       12544       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 64, 80)       0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64, 80)       6480        permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 80, 64)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 80, 64)       0           bidirectional_1[0][0]            \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 5120)         0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            15363       flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 49,939\n",
      "Trainable params: 49,939\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 41173 samples, validate on 4575 samples\n",
      "Epoch 1/50\n",
      "41173/41173 [==============================] - 147s 4ms/step - loss: 1.0104 - acc: 0.4494 - val_loss: 0.9875 - val_acc: 0.4656\n",
      "Epoch 2/50\n",
      "41173/41173 [==============================] - 146s 4ms/step - loss: 0.9787 - acc: 0.4901 - val_loss: 0.9585 - val_acc: 0.5250\n",
      "Epoch 3/50\n",
      "41173/41173 [==============================] - 140s 3ms/step - loss: 0.9566 - acc: 0.5242 - val_loss: 0.9422 - val_acc: 0.5434\n",
      "Epoch 4/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.9425 - acc: 0.5369 - val_loss: 0.9372 - val_acc: 0.5484\n",
      "Epoch 5/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.9334 - acc: 0.5474 - val_loss: 0.9177 - val_acc: 0.5611\n",
      "Epoch 6/50\n",
      "41173/41173 [==============================] - 140s 3ms/step - loss: 0.9246 - acc: 0.5511 - val_loss: 0.9127 - val_acc: 0.5646\n",
      "Epoch 7/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.9141 - acc: 0.5572 - val_loss: 0.9165 - val_acc: 0.5668\n",
      "Epoch 8/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.9059 - acc: 0.5647 - val_loss: 0.9045 - val_acc: 0.5711\n",
      "Epoch 9/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.8958 - acc: 0.5692 - val_loss: 0.9307 - val_acc: 0.5528\n",
      "Epoch 10/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.8878 - acc: 0.5745 - val_loss: 0.8880 - val_acc: 0.5838\n",
      "Epoch 11/50\n",
      "41173/41173 [==============================] - 140s 3ms/step - loss: 0.8830 - acc: 0.5784 - val_loss: 0.8871 - val_acc: 0.5830\n",
      "Epoch 12/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.8769 - acc: 0.5839 - val_loss: 0.8800 - val_acc: 0.5867\n",
      "Epoch 13/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.8706 - acc: 0.5870 - val_loss: 0.8909 - val_acc: 0.5864\n",
      "Epoch 14/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.8618 - acc: 0.5913 - val_loss: 0.8905 - val_acc: 0.5777\n",
      "Epoch 15/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.8574 - acc: 0.5951 - val_loss: 0.9020 - val_acc: 0.5777\n",
      "Epoch 16/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.8534 - acc: 0.5959 - val_loss: 0.8917 - val_acc: 0.5775\n",
      "Epoch 17/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.8481 - acc: 0.5980 - val_loss: 0.8969 - val_acc: 0.5781\n",
      "Epoch 18/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.8430 - acc: 0.6038 - val_loss: 0.8887 - val_acc: 0.5729\n",
      "Epoch 19/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.8396 - acc: 0.6036 - val_loss: 0.8669 - val_acc: 0.5878\n",
      "Epoch 20/50\n",
      "41173/41173 [==============================] - 138s 3ms/step - loss: 0.8352 - acc: 0.6089 - val_loss: 0.9086 - val_acc: 0.5777\n",
      "Epoch 21/50\n",
      "41173/41173 [==============================] - 138s 3ms/step - loss: 0.8326 - acc: 0.6086 - val_loss: 0.8738 - val_acc: 0.5823\n",
      "Epoch 22/50\n",
      "41173/41173 [==============================] - 138s 3ms/step - loss: 0.8307 - acc: 0.6099 - val_loss: 0.8732 - val_acc: 0.5860\n",
      "Epoch 23/50\n",
      "41173/41173 [==============================] - 138s 3ms/step - loss: 0.8233 - acc: 0.6150 - val_loss: 0.8916 - val_acc: 0.5720\n",
      "Epoch 24/50\n",
      "41173/41173 [==============================] - 138s 3ms/step - loss: 0.8216 - acc: 0.6165 - val_loss: 0.8876 - val_acc: 0.5773\n",
      "Epoch 25/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.8185 - acc: 0.6166 - val_loss: 0.9084 - val_acc: 0.5705\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41173/41173 [==============================] - 138s 3ms/step - loss: 0.8135 - acc: 0.6197 - val_loss: 0.8703 - val_acc: 0.5895\n",
      "Epoch 27/50\n",
      "41173/41173 [==============================] - 138s 3ms/step - loss: 0.8124 - acc: 0.6194 - val_loss: 0.8887 - val_acc: 0.5814\n",
      "Epoch 28/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.8108 - acc: 0.6192 - val_loss: 0.8532 - val_acc: 0.5976\n",
      "Epoch 29/50\n",
      "41173/41173 [==============================] - 138s 3ms/step - loss: 0.8050 - acc: 0.6239 - val_loss: 0.8891 - val_acc: 0.5869\n",
      "Epoch 30/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.8015 - acc: 0.6260 - val_loss: 0.8647 - val_acc: 0.5945\n",
      "Epoch 31/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.7999 - acc: 0.6275 - val_loss: 0.8909 - val_acc: 0.5777\n",
      "Epoch 32/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.7980 - acc: 0.6310 - val_loss: 0.8646 - val_acc: 0.5930\n",
      "Epoch 33/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.7968 - acc: 0.6316 - val_loss: 0.8642 - val_acc: 0.5943\n",
      "Epoch 34/50\n",
      "41173/41173 [==============================] - 140s 3ms/step - loss: 0.7919 - acc: 0.6337 - val_loss: 0.8616 - val_acc: 0.5895\n",
      "Epoch 35/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.7913 - acc: 0.6326 - val_loss: 0.8655 - val_acc: 0.5961\n",
      "Epoch 36/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.7862 - acc: 0.6344 - val_loss: 0.8710 - val_acc: 0.5937\n",
      "Epoch 37/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.7841 - acc: 0.6373 - val_loss: 0.8838 - val_acc: 0.5897\n",
      "Epoch 38/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.7838 - acc: 0.6361 - val_loss: 0.8705 - val_acc: 0.5910\n",
      "Epoch 39/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.7807 - acc: 0.6363 - val_loss: 0.8652 - val_acc: 0.5880\n",
      "Epoch 40/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.7788 - acc: 0.6373 - val_loss: 0.8773 - val_acc: 0.5867\n",
      "Epoch 41/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.7761 - acc: 0.6405 - val_loss: 0.8753 - val_acc: 0.5893\n",
      "Epoch 42/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.7708 - acc: 0.6445 - val_loss: 0.8675 - val_acc: 0.5972\n",
      "Epoch 43/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.7698 - acc: 0.6455 - val_loss: 0.8846 - val_acc: 0.5858\n",
      "Epoch 44/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.7657 - acc: 0.6471 - val_loss: 0.8779 - val_acc: 0.5882\n",
      "Epoch 45/50\n",
      "41173/41173 [==============================] - 138s 3ms/step - loss: 0.7661 - acc: 0.6483 - val_loss: 0.8735 - val_acc: 0.5891\n",
      "Epoch 46/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.7642 - acc: 0.6493 - val_loss: 0.8731 - val_acc: 0.5854\n",
      "Epoch 47/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.7567 - acc: 0.6528 - val_loss: 0.8711 - val_acc: 0.5921\n",
      "Epoch 48/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.7566 - acc: 0.6545 - val_loss: 0.8788 - val_acc: 0.5897\n",
      "Epoch 49/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.7533 - acc: 0.6544 - val_loss: 0.8938 - val_acc: 0.5823\n",
      "Epoch 50/50\n",
      "41173/41173 [==============================] - 139s 3ms/step - loss: 0.7526 - acc: 0.6550 - val_loss: 0.8808 - val_acc: 0.5906\n"
     ]
    }
   ],
   "source": [
    "scores=[]\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "for i,t in enumerate(tasks):\n",
    "    model_pol=models[i][0]\n",
    "    model_pol.summary()\n",
    "    \n",
    "    model_pol.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    X_polarity_train_=sequence.pad_sequences(X_polarity_train[i], maxlen=maxlen)\n",
    "   \n",
    "    model_pol.fit(X_polarity_train_,y_polarity_train[i],\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs_polarity,\n",
    "                  validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating task taskA\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 80)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 80, 32)       3200        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 80, 32)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 80, 16)       1552        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 80, 16)       2576        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 80, 16)       3600        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 80, 16)       4624        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 20, 16)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 20, 16)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 20, 16)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 20, 16)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 80, 16)       0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 80, 64)       12544       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 64, 80)       0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64, 80)       6480        permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 80, 64)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 80, 64)       0           bidirectional_1[0][0]            \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 5120)         0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            5121        flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 39,697\n",
      "Trainable params: 39,697\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 11s 4ms/step - loss: 0.6886 - acc: 0.5594\n",
      "1151/1151 [==============================] - 1s 1ms/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.32      0.44       577\n",
      "          1       0.56      0.86      0.68       574\n",
      "\n",
      "avg / total       0.63      0.59      0.56      1151\n",
      "\n",
      "Macro f-score: 0.5592286434183215\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 9s 3ms/step - loss: 0.6750 - acc: 0.5863\n",
      "1151/1151 [==============================] - 1s 776us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.59      0.60       577\n",
      "          1       0.60      0.61      0.61       574\n",
      "\n",
      "avg / total       0.60      0.60      0.60      1151\n",
      "\n",
      "Macro f-score: 0.6029347712036426\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 9s 3ms/step - loss: 0.6545 - acc: 0.6280\n",
      "1151/1151 [==============================] - 1s 768us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.63      0.61       577\n",
      "          1       0.60      0.56      0.58       574\n",
      "\n",
      "avg / total       0.60      0.60      0.60      1151\n",
      "\n",
      "Macro f-score: 0.5955149671138399\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 9s 3ms/step - loss: 0.6380 - acc: 0.6422\n",
      "1151/1151 [==============================] - 1s 773us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.57      0.59       577\n",
      "          1       0.59      0.63      0.61       574\n",
      "\n",
      "avg / total       0.60      0.60      0.60      1151\n",
      "\n",
      "Macro f-score: 0.6017168640108312\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 9s 3ms/step - loss: 0.6293 - acc: 0.6448\n",
      "1151/1151 [==============================] - 1s 776us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.64      0.61       577\n",
      "          1       0.60      0.55      0.57       574\n",
      "\n",
      "avg / total       0.60      0.60      0.59      1151\n",
      "\n",
      "Macro f-score: 0.5941393149872272\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 9s 3ms/step - loss: 0.6126 - acc: 0.6660\n",
      "1151/1151 [==============================] - 1s 782us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.66      0.62       577\n",
      "          1       0.62      0.56      0.58       574\n",
      "\n",
      "avg / total       0.61      0.61      0.60      1151\n",
      "\n",
      "Macro f-score: 0.6045212363394181\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 9s 3ms/step - loss: 0.6015 - acc: 0.6668\n",
      "1151/1151 [==============================] - 1s 768us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.61      0.61       577\n",
      "          1       0.61      0.62      0.62       574\n",
      "\n",
      "avg / total       0.61      0.61      0.61      1151\n",
      "\n",
      "Macro f-score: 0.6133691635875043\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 9s 3ms/step - loss: 0.5882 - acc: 0.6895\n",
      "1151/1151 [==============================] - 1s 765us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.60      0.61       577\n",
      "          1       0.61      0.63      0.62       574\n",
      "\n",
      "avg / total       0.62      0.62      0.62      1151\n",
      "\n",
      "Macro f-score: 0.6176403082795777\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 9s 3ms/step - loss: 0.5850 - acc: 0.6813\n",
      "1151/1151 [==============================] - 1s 763us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.59      0.61       577\n",
      "          1       0.61      0.65      0.63       574\n",
      "\n",
      "avg / total       0.62      0.62      0.62      1151\n",
      "\n",
      "Macro f-score: 0.6200364562913543\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2683/2683 [==============================] - 9s 3ms/step - loss: 0.5751 - acc: 0.6970\n",
      "1151/1151 [==============================] - 1s 767us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.73      0.65       577\n",
      "          1       0.65      0.50      0.57       574\n",
      "\n",
      "avg / total       0.62      0.62      0.61      1151\n",
      "\n",
      "Macro f-score: 0.6101440399540943\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i,t in enumerate(tasks):\n",
    "    print(\"Evaluating task\",t)\n",
    "    model_ir=models[i][1]\n",
    "\n",
    "    model_ir.summary()\n",
    "    \n",
    "    if t==\"taskA\":\n",
    "        model_ir.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    else:\n",
    "        model_ir.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    X_train_=sequence.pad_sequences(X_train[i], maxlen=maxlen)\n",
    "    X_test_=sequence.pad_sequences(X_test[i], maxlen=maxlen)\n",
    "    \n",
    "\n",
    "    for it in range(epochs_irony):\n",
    "        model_ir.fit(X_train_,y_train[i],\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=1,\n",
    "                  validation_split=0)\n",
    "    \n",
    "    \n",
    "        #score, acc = model.evaluate(X_test_,y_test[i],batch_size=batch_size)\n",
    "    \n",
    "        y_test_ = model_ir.predict(X_test_, batch_size=batch_size, verbose=1)\n",
    "        if t==\"taskA\":\n",
    "            y_test_ = np.round(y_test_)\n",
    "            ori = y_test[i]\n",
    "        else:\n",
    "            y_test_ = np.argmax(y_test_, axis=1)\n",
    "            ori = np.argmax(y_test[i],axis=1)\n",
    "        \n",
    "        print(classification_report(ori, y_test_))\n",
    "        print(\"Macro f-score:\", f1_score(ori, y_test_, average=\"macro\"))\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
