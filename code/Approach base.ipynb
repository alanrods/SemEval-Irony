{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os.path\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Bidirectional, Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import SpatialDropout1D, Concatenate, Maximum\n",
    "\n",
    "max_features = 100\n",
    "maxlen = 80\n",
    "embedding_size = 32\n",
    "lstm_output_size = 32\n",
    "batch_size = 128\n",
    "epochs_polarity = 100\n",
    "epochs_irony= 10\n",
    "\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos ironía\n",
    "\n",
    "Se cargan los datos de ironía, por cada tarea:\n",
    "\n",
    "* taskA: Clasificador binario\n",
    "* taskB: Clasificador multi-clase (4?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening files\n",
      "../SemEval2018-Task3/infotec_train_dev/SemEval2018-T3-taskA_train.json\n",
      "Size 2683\n",
      "../SemEval2018-Task3/infotec_train_dev/SemEval2018-T3-taskB_train.json\n",
      "Size 2683\n",
      "Opening files\n",
      "../SemEval2018-Task3/infotec_train_dev/SemEval2018-T3-taskA_test.json\n",
      "Size 1151\n",
      "../SemEval2018-Task3/infotec_train_dev/SemEval2018-T3-taskB_test.json\n",
      "Size 1151\n"
     ]
    }
   ],
   "source": [
    "def load_files(files):\n",
    "    json_files=[]\n",
    "    print(\"Opening files\")\n",
    "    for filename in files:\n",
    "        print(filename)\n",
    "        data=[]\n",
    "        for line in codecs.open(filename):\n",
    "            data.append(json.loads(line))\n",
    "        json_files.append(data)\n",
    "        print(\"Size\",len(json_files[-1]))\n",
    "    return json_files\n",
    "\n",
    "\n",
    "tasks=[\"taskA\",\"taskB\"]\n",
    "dirname=\"../SemEval2018-Task3/infotec_train_dev\"\n",
    "basename=\"SemEval2018-T3-{0}_{1}.json\"\n",
    "train_files=[os.path.join(dirname,basename.format(task,'train')) for task in tasks]\n",
    "test_files=[os.path.join(dirname,basename.format(task,'test')) for task in tasks]\n",
    "\n",
    "train_json=load_files(train_files)\n",
    "test_json=load_files(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos de polaridad\n",
    "\n",
    "Cargando datos de polaridad, una sola tarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening files\n",
      "../extras/En.json\n",
      "Size 45748\n"
     ]
    }
   ],
   "source": [
    "train_polarity_json=load_files([\"../extras/En.json\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 2683\n",
      "Size: 1151\n",
      "Size: 2683\n",
      "Size: 1151\n",
      "Size: 45748\n"
     ]
    }
   ],
   "source": [
    "# Configurando datos de ironía\n",
    "\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "nltk_tok=TweetTokenizer(reduce_len=True)\n",
    "\n",
    "data_train=[]\n",
    "data_test=[]\n",
    "for i,t in enumerate(tasks):\n",
    "    text_train=[\" \".join(nltk_tok.tokenize(j['text'])) for j in train_json[i]]\n",
    "    class_train=[j['klass'] for j in train_json[i]]\n",
    "    data_train.append(list(zip(text_train,class_train)))\n",
    "    text_test=[\" \".join(nltk_tok.tokenize(j['text'])) for j in test_json[i]]\n",
    "    class_test=[j['klass'] for j in test_json[i]]\n",
    "    data_test.append(list(zip(text_test,class_test)))\n",
    "\n",
    "\n",
    "for i,t in enumerate(data_train):\n",
    "    print(\"Size:\",len(data_train[i]))\n",
    "    print(\"Size:\",len(data_test[i]))\n",
    "    \n",
    "# Configurando datos de polaridad\n",
    "\n",
    "def pol2class(k):\n",
    "    onehot=[[1,0,0],[0,1,0],[0,0,1]]\n",
    "    return onehot[['neutral','positive','negative'].index(k)]\n",
    "\n",
    "text_polarity_train=[\" \".join(nltk_tok.tokenize(j['text'])) for j in train_polarity_json]\n",
    "class_polarity_train=[pol2class(j['klass']) for j in train_polarity_json]\n",
    "data_polarity_train=list(zip(text_polarity_train,class_polarity_train))\n",
    "\n",
    "print(\"Size:\",len(data_polarity_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "  \n",
    "\n",
    "def normalizeText(tweet):\n",
    "    #tweet = re.sub(r'#(S|s)arcasm|#(I|i)rony','',tweet)\n",
    "    #tweet = re.sub(r'#SARCASM|#IRONY','',tweet)\n",
    "    tweet = re.sub(r'https?://t\\.co/.(\\w|\\d)+','http', tweet) #tweet link\n",
    "    tweet = re.sub(r'fb\\.me/.(\\w|\\d)+','fb', tweet) #tweet link\n",
    "    tweet = re.sub(r'https?://.+','http', tweet) #tweet link\n",
    "    tweet = re.sub(r'@.\\w*','@',tweet)\n",
    "    #tweet = re.sub(r'#','',tweet)\n",
    "    return tweet\n",
    "\n",
    "def buildTokenizer(tweets):\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_features,lower=False, filters='\\t\\n', split=\" \",char_level=True )\n",
    "    tokenizer.fit_on_texts([\" \".join(nltk_tok.tokenize(t)) for t in tweets])\n",
    "    return tokenizer\n",
    "\n",
    "def text2seq(tok,tweet):\n",
    "    return tok.texts_to_sequences([normalizeText(tweet)])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_counts 135\n",
      "word_docs 135\n",
      "word_index 135\n",
      "Top words\n",
      "   (1009425) , e (392998) , t (341380) , a (309908) , o (302099) , n (255361) , i (244677) , r (214410) , s (211597) , h (183558) , l (147455) , d (129392) , u (107501) , y (100262) , m (99122) , g (84966) , c (83140) , w (75137) , . (68693) , p (68296) , f (62750) , b (58628) , k (42424) , v (34797) , S (30490) , I (29341) , ' (25133) , T (24650) , , (23257) , A (20543) , M (20280) , C (19217) , @ (19209) , B (17370) , ! (16269) , D (14741) , # (14525) , W (14130) , 1 (14067) , N (13406) , P (13032) , F (12931) , R (12859) , H (12329) , O (11890) , G (11358) , L (11191) , E (10907) , 2 (10893) , : (10709) \n",
      "Last words\n",
      " > (311) , < (292) , % (272) , = (258) , ] (207) , [ (206) , ~ (200) , ^ (142) , ` (35) , \\ (33) , ️ (25) , } (9) , – (8) , { (7) , £ (5) , \n",
      " (5) , ’ (3) , ☆ (3) , ・ (3) , — (2) , Ｏ (2) , Ｌ (2) , İ (2) , ℃ (2) , 你 (2) , ï (1) , 󾍁 (1) , ° (1) , ง (1) , ว (1) , ย (1) , Ｆ (1) , Ｗ (1) , « (1) , ó (1) , í (1) , ⁰ (1) , Ã (1) , ¢ (1) , Ë (1) , œ (1) , Å (1) , ö (1) , 就 (1) , 算 (1) , 隔 (1) , 格 (1) , 我 (1) , 都 (1) , 知 (1) \n",
      "word_counts 134\n",
      "word_docs 134\n",
      "word_index 134\n",
      "Top words\n",
      "   (1009595) , e (393144) , t (341425) , a (309955) , o (302156) , n (255348) , i (244768) , r (214469) , s (211632) , h (183616) , l (147482) , d (129381) , u (107569) , y (100294) , m (99129) , g (84980) , c (83178) , w (75134) , . (68748) , p (68306) , f (62785) , b (58685) , k (42408) , v (34813) , S (30503) , I (29342) , ' (25149) , T (24632) , , (23244) , A (20557) , M (20282) , C (19223) , @ (19198) , B (17364) , ! (16265) , D (14735) , # (14515) , W (14136) , 1 (14081) , N (13405) , P (13047) , F (12927) , R (12862) , H (12307) , O (11899) , G (11350) , L (11200) , E (10901) , 2 (10900) , : (10679) \n",
      "Last words\n",
      " + (462) , > (308) , < (293) , % (270) , = (256) , [ (205) , ] (205) , ~ (198) , ^ (143) , \\ (35) , ` (35) , ️ (27) , } (9) , – (8) , £ (7) , { (7) , \n",
      " (5) , ’ (3) , ☆ (3) , ・ (3) , — (2) , Ｏ (2) , Ｌ (2) , 你 (2) , ° (2) , İ (2) , ℃ (2) , ó (2) , ï (1) , Ｆ (1) , Ｗ (1) , ง (1) , ว (1) , ย (1) , 就 (1) , 算 (1) , 隔 (1) , 格 (1) , 我 (1) , 都 (1) , 知 (1) , 讲 (1) , « (1) , í (1) , ⁰ (1) , Ã (1) , ¢ (1) , Ë (1) , œ (1) , Å (1) \n"
     ]
    }
   ],
   "source": [
    "toks=[]\n",
    "maps_=[]\n",
    "for i,t in enumerate(tasks):\n",
    "    tweets=[t for t,c in data_train[i]]\n",
    "    tweets_=[t for t,c in data_polarity_train]\n",
    "    toks.append(buildTokenizer([normalizeText(t) for t in tweets+tweets_]))\n",
    "    maps_.append({v: k for k, v in toks[-1].word_index.items()})\n",
    "    print(\"word_counts\",len(toks[-1].word_counts))\n",
    "    print(\"word_docs\",len(toks[-1].word_docs))\n",
    "    print(\"word_index\",len(toks[-1].word_index))\n",
    "    print(\"Top words\\n\",\", \".join([\"{0} ({1}) \".format(maps_[-1][f+1],toks[i].word_counts[maps_[-1][f+1]]) for f in range(50)]))\n",
    "    print(\"Last words\\n\",\", \".join([\"{0} ({1}) \".format(maps_[-1][f],toks[i].word_counts[maps_[-1][f]]) for f in range(len(maps_[i])-50,len(maps_[i]))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example train in taskA\n",
      "The mouse's first incepted memory was just the sound : BRAAAWWWP !\n",
      "T h e   m o u s e ' s   f i r s t   i n c e p t e d   m e m o r y   w a s   j u s t   t h e   s o u n d   :   B R A A A W W W P   !\n",
      ">>I LOVE not sleeping . It's the best .\n",
      "I   L O V E   n o t   s l e e p i n g   .   I t ' s   t h e   b e s t   .\n",
      ">>Religion is unfounded , else , Allah would have saved the kids . . @tariqmushtaqkh @nicpradhan #PeshawarAttack #PakSchoolSiege\n",
      "R e l i g i o n   i s   u n f o u n d e d   ,   e l s e   ,   A l l a h   w o u l d   h a v e   s a v e d   t h e   k i d s   .   .   @   @   # P e s h a w a r A t t a c k   # P a k S c h o o l S i e g e\n",
      ">>Love how I came into work at 8 because Charlie said we were busy ... 3 people in 45 minutes , yeah we got this place packed Charlie .\n",
      "L o v e   h o w   I   c a m e   i n t o   w o r k   a t   8   b e c a u s e   C h a r l i e   s a i d   w e   w e r e   b u s y   . . .   3   p e o p l e   i n   4 5   m i n u t e s   ,   y e a h   w e   g o t   t h i s   p l a c e   p a c k e d   C h a r l i e   .\n",
      ">>Thx for catching on #urock\n",
      "T h x   f o r   c a t c h i n g   o n   # u r o c k\n",
      ">>@DFDSUKUpdates why all the delays ? happy\n",
      "@   w h y   a l l   t h e   d e l a y s   ?   h a p p y\n",
      ">>I am extremely excited to know what Reza Aslan and CJ Werleman think about what happened in Pakistan #PeshawarAttack\n",
      "I   a m   e x t r e m e l y   e x c i t e d   t o   k n o w   w h a t   R e z a   A s l a n   a n d   C J   W e r l e m a n   t h i n k   a b o u t   w h a t   h a p p e n e d   i n   P a k i s t a n   # P e s h a w a r A t t a c k\n",
      ">>Kebabs ordered .. | Snug on sofa watching shit tele with dad : two_hearts :: purple_heart :\n",
      "K e b a b s   o r d e r e d   . .   |   S n u g   o n   s o f a   w a t c h i n g   s h i t   t e l e   w i t h   d a d   :   t w o _ h e a r t s   : :   p u r p l e _ h e a r t   :\n",
      ">>Still feeling sorry for myself about being to ill to go out last night ! ! ! : loudly_crying_face :: crying_face : here's my memories from ... http://t.co/I3fIk1k6ek\n",
      "S t i l l   f e e l i n g   s o r r y   f o r   m y s e l f   a b o u t   b e i n g   t o   i l l   t o   g o   o u t   l a s t   n i g h t   !   !   !   :   l o u d l y _ c r y i n g _ f a c e   : :   c r y i n g _ f a c e   :   h e r e ' s   m y   m e m o r i e s   f r o m   . . .   h t t p\n",
      ">>@quiksilverindia The only place where you don't wanna sit on the seat that you earned . | #Dancing | #QuiksilverGoesSupersonic\n",
      "@   T h e   o n l y   p l a c e   w h e r e   y o u   d o n ' t   w a n n a   s i t   o n   t h e   s e a t   t h a t   y o u   e a r n e d   .   |   # D a n c i n g   |   # Q u i k s i l v e r G o e s S u p e r s o n i c\n",
      ">>Accidentally breaking your own computer when your tech support ! Haha , talk about employee of the month #techSupport :p ersonal_computer :: floppy_disk :: pistol :\n",
      "A c c i d e n t a l l y   b r e a k i n g   y o u r   o w n   c o m p u t e r   w h e n   y o u r   t e c h   s u p p o r t   !   H a h a   ,   t a l k   a b o u t   e m p l o y e e   o f   t h e   m o n t h   # t e c h S u p p o r t   : p   e r s o n a l _ c o m p u t e r   : :   f l o p p y _ d i s k   : :   p i s t o l   :\n",
      ">>BITCH WHERE THE FUCK U THINK IM GOING ? ? ? | BETTER SIT ON DOWN WITH THE REST OF THESE BITCHES WAITTNG FOR ME TO MOVE ... NEVA\n",
      "B I T C H   W H E R E   T H E   F U C K   U   T H I N K   I M   G O I N G   ?   ?   ?   |   B E T T E R   S I T   O N   D O W N   W I T H   T H E   R E S T   O F   T H E S E   B I T C H E S   W A I T T N G   F O R   M E   T O   M O V E   . . .   N E V A\n",
      ">>Hey everyone ... I want this as a Christmas gift :) Can you send me one ? @NVIDIAGeForce #GTX980 http://t.co/nc7S4QOqpW\n",
      "H e y   e v e r y o n e   . . .   I   w a n t   t h i s   a s   a   C h r i s t m a s   g i f t   : )   C a n   y o u   s e n d   m e   o n e   ?   @   # G T X 9 8 0   h t t p\n",
      ">>2015 prediction : Putin ditches the Ruble and adopts Bitcoin\n",
      "2 0 1 5   p r e d i c t i o n   :   P u t i n   d i t c h e s   t h e   R u b l e   a n d   a d o p t s   B i t c o i n\n",
      ">>#nuffsaid #stupidity #hadenough lols : face_with_tears_of_joy :: smiling_face_with_open_mouth : http://t.co/0iSZ1ExuLn\n",
      "# n u f f s a i d   # s t u p i d i t y   # h a d e n o u g h   l o l s   :   f a c e _ w i t h _ t e a r s _ o f _ j o y   : :   s m i l i n g _ f a c e _ w i t h _ o p e n _ m o u t h   :   h t t p\n",
      ">>alot-still not sussed out how yo keep in touch with peop ! e via twitterTwitterversary\n",
      "a l o t - s t i l l   n o t   s u s s e d   o u t   h o w   y o   k e e p   i n   t o u c h   w i t h   p e o p   !   e   v i a   t w i t t e r T w i t t e r v e r s a r y\n",
      ">>Thanks mum for farting ... Not once , but twice during dinner with the BF . #NiceOne\n",
      "T h a n k s   m u m   f o r   f a r t i n g   . . .   N o t   o n c e   ,   b u t   t w i c e   d u r i n g   d i n n e r   w i t h   t h e   B F   .   # N i c e O n e\n",
      ">>Shoes on make-up done ! I'm ready\n",
      "S h o e s   o n   m a k e - u p   d o n e   !   I ' m   r e a d y\n",
      ">>@Bungie @DestinyTheGame thanks for plan c two weeks in a row ! Getting rid of exotic engrams is also an amazing move !\n",
      "@   @   t h a n k s   f o r   p l a n   c   t w o   w e e k s   i n   a   r o w   !   G e t t i n g   r i d   o f   e x o t i c   e n g r a m s   i s   a l s o   a n   a m a z i n g   m o v e   !\n",
      ">>@darrellr79 @EBJunkies I'd prefer the slew of #Ferguson protestors across the US ... they stop traffic shut dwn bridges\n",
      "@   @   I ' d   p r e f e r   t h e   s l e w   o f   # F e r g u s o n   p r o t e s t o r s   a c r o s s   t h e   U S   . . .   t h e y   s t o p   t r a f f i c   s h u t   d w n   b r i d g e s\n",
      ">>@LOLGOP Fox News criticizing poor journalism regarding the UVA incident is hilarious\n",
      "@   F o x   N e w s   c r i t i c i z i n g   p o o r   j o u r n a l i s m   r e g a r d i n g   t h e   U V A   i n c i d e n t   i s   h i l a r i o u s\n",
      ">>I just love a chaotic school run ! ! ! ...\n",
      "I   j u s t   l o v e   a   c h a o t i c   s c h o o l   r u n   !   !   !   . . .\n",
      ">>I'd rather you know and be upset than hide it for you to only question things later\n",
      "I ' d   r a t h e r   y o u   k n o w   a n d   b e   u p s e t   t h a n   h i d e   i t   f o r   y o u   t o   o n l y   q u e s t i o n   t h i n g s   l a t e r\n",
      ">>@TaylorLynn0022 | Ew you really did it ?\n",
      "@   |   E w   y o u   r e a l l y   d i d   i t   ?\n",
      ">>Left my lunch at home . Swansea canteen outdoing themselves with this generous and well priced portion of food http://t.co/jn7Kff2K18\n",
      "L e f t   m y   l u n c h   a t   h o m e   .   S w a n s e a   c a n t e e n   o u t d o i n g   t h e m s e l v e s   w i t h   t h i s   g e n e r o u s   a n d   w e l l   p r i c e d   p o r t i o n   o f   f o o d   h t t p\n",
      ">>@haleymae21 Ha ! I don't have haters . Just a few misguided souls who thought my mentions were a bulletin board for their issues . ;)\n",
      "@   H a   !   I   d o n ' t   h a v e   h a t e r s   .   J u s t   a   f e w   m i s g u i d e d   s o u l s   w h o   t h o u g h t   m y   m e n t i o n s   w e r e   a   b u l l e t i n   b o a r d   f o r   t h e i r   i s s u e s   .   ; )\n",
      ">>@CarlosDenWA great Christmas present . Unreal what the alcohol industry will do to lure young people . #sad #nosocialconscience\n",
      "@   g r e a t   C h r i s t m a s   p r e s e n t   .   U n r e a l   w h a t   t h e   a l c o h o l   i n d u s t r y   w i l l   d o   t o   l u r e   y o u n g   p e o p l e   .   # s a d   # n o s o c i a l c o n s c i e n c e\n",
      ">>@jcpetit4 dusty as usual : electric_plug : #redbucket http://t.co/U8llvttn59\n",
      "@   d u s t y   a s   u s u a l   :   e l e c t r i c _ p l u g   :   # r e d b u c k e t   h t t p\n",
      ">>@Sahelanth @QuerierNew @hudds1 because as long as she's a victim she'll have white knights trying to save that damsel in distress .\n",
      "@   @   @   b e c a u s e   a s   l o n g   a s   s h e ' s   a   v i c t i m   s h e ' l l   h a v e   w h i t e   k n i g h t s   t r y i n g   t o   s a v e   t h a t   d a m s e l   i n   d i s t r e s s   .\n",
      ">>@steigerwaldino Nah , it's better we all act like the North Korean govt and police people's private thoughts .\n",
      "@   N a h   ,   i t ' s   b e t t e r   w e   a l l   a c t   l i k e   t h e   N o r t h   K o r e a n   g o v t   a n d   p o l i c e   p e o p l e ' s   p r i v a t e   t h o u g h t s   .\n",
      ">>A voice of reason crying in the wilderness of hysteria . http://t.co/GHWycSm5bW via @sharethis\n",
      "A   v o i c e   o f   r e a s o n   c r y i n g   i n   t h e   w i l d e r n e s s   o f   h y s t e r i a   .   h t t p   v i a   @\n",
      ">>Towing company commuter car being towed . http://t.co/O3NbsLQahp\n",
      "T o w i n g   c o m p a n y   c o m m u t e r   c a r   b e i n g   t o w e d   .   h t t p\n",
      ">>@SouthamptonFC @LFC @SkySports Again fans come second . Thanks again\n",
      "@   @   @   A g a i n   f a n s   c o m e   s e c o n d   .   T h a n k s   a g a i n\n",
      ">>Seems as if @ProudMaryBoise wants to endorse me on LinkedIn for - any thoughts on this from the #OMCchat crowd ?\n",
      "S e e m s   a s   i f   @   w a n t s   t o   e n d o r s e   m e   o n   L i n k e d I n   f o r   -   a n y   t h o u g h t s   o n   t h i s   f r o m   t h e   # O M C c h a t   c r o w d   ?\n",
      ">>Thank you @angiegalifas for these amazing candles from the new series of @victoriassecret #atmosphere ... http://t.co/JOhwVxVH0b\n",
      "T h a n k   y o u   @   f o r   t h e s e   a m a z i n g   c a n d l e s   f r o m   t h e   n e w   s e r i e s   o f   @   # a t m o s p h e r e   . . .   h t t p\n",
      ">>father sent me down to buy ingredients cos he wanna fry some noodles up i remembered everything else but the noodles #somuchwin\n",
      "f a t h e r   s e n t   m e   d o w n   t o   b u y   i n g r e d i e n t s   c o s   h e   w a n n a   f r y   s o m e   n o o d l e s   u p   i   r e m e m b e r e d   e v e r y t h i n g   e l s e   b u t   t h e   n o o d l e s   # s o m u c h w i n\n",
      ">>@IkeMagnifico Funny thing is now she's a security guard at Fed Ex in LA where I pick up all the time .\n",
      "@   F u n n y   t h i n g   i s   n o w   s h e ' s   a   s e c u r i t y   g u a r d   a t   F e d   E x   i n   L A   w h e r e   I   p i c k   u p   a l l   t h e   t i m e   .\n",
      ">>Packers fans on Twitter are on full meltdown mode ... R . E . L . A . X .\n",
      "P a c k e r s   f a n s   o n   T w i t t e r   a r e   o n   f u l l   m e l t d o w n   m o d e   . . .   R   .   E   .   L   .   A   .   X   .\n",
      ">>@RandBall gotta hand it to them , really . : clapping_hands_sign :: clapping_hands_sign :: clapping_hands_sign :\n",
      "@   g o t t a   h a n d   i t   t o   t h e m   ,   r e a l l y   .   :   c l a p p i n g _ h a n d s _ s i g n   : :   c l a p p i n g _ h a n d s _ s i g n   : :   c l a p p i n g _ h a n d s _ s i g n   :\n",
      ">>@KevinnCyrus lol of course it's the best ! !\n",
      "@   l o l   o f   c o u r s e   i t ' s   t h e   b e s t   !   !\n",
      ">>@KevinStryker What do you know about #iubb ? Tom Crean is a coaching genius !\n",
      "@   W h a t   d o   y o u   k n o w   a b o u t   # i u b b   ?   T o m   C r e a n   i s   a   c o a c h i n g   g e n i u s   !\n",
      ">>@Moose_eBooks You seriously bought a giant TV to play in the box ?\n",
      "@   Y o u   s e r i o u s l y   b o u g h t   a   g i a n t   T V   t o   p l a y   i n   t h e   b o x   ?\n",
      ">>the people #BillCosby looked down upon are ones coming to his defense . This isn't about race , it's about rape ! http://t.co/PD4GUCPPKV\n",
      "t h e   p e o p l e   # B i l l C o s b y   l o o k e d   d o w n   u p o n   a r e   o n e s   c o m i n g   t o   h i s   d e f e n s e   .   T h i s   i s n ' t   a b o u t   r a c e   ,   i t ' s   a b o u t   r a p e   !   h t t p\n",
      ">>they don't sing live , but they sure are hella good looking #smh\n",
      "t h e y   d o n ' t   s i n g   l i v e   ,   b u t   t h e y   s u r e   a r e   h e l l a   g o o d   l o o k i n g   # s m h\n",
      ">>I have such a loving family\n",
      "I   h a v e   s u c h   a   l o v i n g   f a m i l y\n",
      ">>@Monaiza_Diva kabhi I'm messier than usual messy wesay :p ersevering_face : I need chai & I'm resting for last 2 days . May be this is the reason .\n",
      "@   k a b h i   I ' m   m e s s i e r   t h a n   u s u a l   m e s s y   w e s a y   : p   e r s e v e r i n g _ f a c e   :   I   n e e d   c h a i   &   I ' m   r e s t i n g   f o r   l a s t   2   d a y s   .   M a y   b e   t h i s   i s   t h e   r e a s o n   .\n",
      ">>@chriscomben @lexpersaud it's in my Priory contract to wear those . Hugely popular with Priory Ultras #does #count #xxx\n",
      "@   @   i t ' s   i n   m y   P r i o r y   c o n t r a c t   t o   w e a r   t h o s e   .   H u g e l y   p o p u l a r   w i t h   P r i o r y   U l t r a s   # d o e s   # c o u n t   # x x x\n",
      ">>I just love 160 question tests in the morning .\n",
      "I   j u s t   l o v e   1 6 0   q u e s t i o n   t e s t s   i n   t h e   m o r n i n g   .\n",
      ">>Si Ms . Educ hehehe clear my ignorance : smiling_face_with_heart-shaped_eyes :: face_with_tears_of_joy :\n",
      "S i   M s   .   E d u c   h e h e h e   c l e a r   m y   i g n o r a n c e   :   s m i l i n g _ f a c e _ w i t h _ h e a r t - s h a p e d _ e y e s   : :   f a c e _ w i t h _ t e a r s _ o f _ j o y   :\n",
      ">>http://t.co/RfwyHOMH2S #fOLLOW THE #MONEY #HOMEbiz NOT #mlm * * * NOTE YOU CAN GET PAID FOR POSTING ON ... http://t.co/Dkr1MjdsTd\n",
      "h t t p   # f O L L O W   T H E   # M O N E Y   # H O M E b i z   N O T   # m l m   *   *   *   N O T E   Y O U   C A N   G E T   P A I D   F O R   P O S T I N G   O N   . . .   h t t p\n",
      "\n",
      "[0, 1, 1, 1, 1]\n",
      "\n",
      "Example train in taskB\n",
      "Produce all kinds of Creative Designs #like15 | http://t.co/OXeuznMhY8 http://t.co/w4eZ9mObFJ\n",
      "P r o d u c e   a l l   k i n d s   o f   C r e a t i v e   D e s i g n s   # l i k e 1 5   |   h t t p   h t t p\n",
      ">>there is only 1 race , HUMAN so i dont look at things by the myth of \" races \" @jtarleta53 @RBRNetwork1\n",
      "t h e r e   i s   o n l y   1   r a c e   ,   H U M A N   s o   i   d o n t   l o o k   a t   t h i n g s   b y   t h e   m y t h   o f   \"   r a c e s   \"   @   @\n",
      ">>@bigbillybmoney oh haha no they ain't : face_with_tears_of_joy :: face_with_tears_of_joy : you ain't even seen em\n",
      "@   o h   h a h a   n o   t h e y   a i n ' t   :   f a c e _ w i t h _ t e a r s _ o f _ j o y   : :   f a c e _ w i t h _ t e a r s _ o f _ j o y   :   y o u   a i n ' t   e v e n   s e e n   e m\n",
      ">>@Only1Neets Probably a Nigerian .\n",
      "@   P r o b a b l y   a   N i g e r i a n   .\n",
      ">>#newjob Urgent CONTRACT - Assurance Manager - Private Banking , Singapore - Not Specified , Singapore Specifie ... http://t.co/bASQ2zWos1\n",
      "# n e w j o b   U r g e n t   C O N T R A C T   -   A s s u r a n c e   M a n a g e r   -   P r i v a t e   B a n k i n g   ,   S i n g a p o r e   -   N o t   S p e c i f i e d   ,   S i n g a p o r e   S p e c i f i e   . . .   h t t p\n",
      ">>My favorite part of dislocating my shoulder is now being able to feel it roll around in the socket\n",
      "M y   f a v o r i t e   p a r t   o f   d i s l o c a t i n g   m y   s h o u l d e r   i s   n o w   b e i n g   a b l e   t o   f e e l   i t   r o l l   a r o u n d   i n   t h e   s o c k e t\n",
      ">>Finals week plus hearing that my grandma is in the ICU = amazing #letmegohome\n",
      "F i n a l s   w e e k   p l u s   h e a r i n g   t h a t   m y   g r a n d m a   i s   i n   t h e   I C U   =   a m a z i n g   # l e t m e g o h o m e\n",
      ">>Ahh gotta love those December electric bills .\n",
      "A h h   g o t t a   l o v e   t h o s e   D e c e m b e r   e l e c t r i c   b i l l s   .\n",
      ">>@UKLabour | | How dare @David_Cameron not pass legislation to force people to catch buses on those routes . Should be a life in jail . | |\n",
      "@   |   |   H o w   d a r e   @   n o t   p a s s   l e g i s l a t i o n   t o   f o r c e   p e o p l e   t o   c a t c h   b u s e s   o n   t h o s e   r o u t e s   .   S h o u l d   b e   a   l i f e   i n   j a i l   .   |   |\n",
      ">>. @BDUTT is at her best when she reports frm ' on the field ' . Not that she is bad at studio but on field she is a true story teller .\n",
      ".   @   i s   a t   h e r   b e s t   w h e n   s h e   r e p o r t s   f r m   '   o n   t h e   f i e l d   '   .   N o t   t h a t   s h e   i s   b a d   a t   s t u d i o   b u t   o n   f i e l d   s h e   i s   a   t r u e   s t o r y   t e l l e r   .\n",
      ">>Come to jhb to play volleyball on a fake beach #work http://t.co/yS6wkgvT5y\n",
      "C o m e   t o   j h b   t o   p l a y   v o l l e y b a l l   o n   a   f a k e   b e a c h   # w o r k   h t t p\n",
      ">>@JafarBalarabe @GovElrufai2015 @elrufai2015 @elrufai for Elrufai #he represents the masses #so it is our victory #\n",
      "@   @   @   @   f o r   E l r u f a i   # h e   r e p r e s e n t s   t h e   m a s s e s   # s o   i t   i s   o u r   v i c t o r y   #\n",
      ">>@tysonmanker @FoxNews Wow . Catch up with the news . \" False intelligence \" found to be not false . See Wikileaks and other sources .\n",
      "@   @   W o w   .   C a t c h   u p   w i t h   t h e   n e w s   .   \"   F a l s e   i n t e l l i g e n c e   \"   f o u n d   t o   b e   n o t   f a l s e   .   S e e   W i k i l e a k s   a n d   o t h e r   s o u r c e s   .\n",
      ">>Togepi do shed their www.MonsterMMORPG . com #shell . fally by #MonsterMMORPG #bagon\n",
      "T o g e p i   d o   s h e d   t h e i r   w w w . M o n s t e r M M O R P G   .   c o m   # s h e l l   .   f a l l y   b y   # M o n s t e r M M O R P G   # b a g o n\n",
      ">>Where I am @ Boys Town http://t.co/dr01zwqGhQ\n",
      "W h e r e   I   a m   @   T o w n   h t t p\n",
      ">>@greateranglia really well done . Another short formation , so I get to stand for the 3rd day in a row from Kel to LLS 7:18 . Thanks\n",
      "@   r e a l l y   w e l l   d o n e   .   A n o t h e r   s h o r t   f o r m a t i o n   ,   s o   I   g e t   t o   s t a n d   f o r   t h e   3 r d   d a y   i n   a   r o w   f r o m   K e l   t o   L L S   7 : 1 8   .   T h a n k s\n",
      ">>@nytimes oh so someone got in trouble for making fun of the people who behead innocent people ... nice\n",
      "@   o h   s o   s o m e o n e   g o t   i n   t r o u b l e   f o r   m a k i n g   f u n   o f   t h e   p e o p l e   w h o   b e h e a d   i n n o c e n t   p e o p l e   . . .   n i c e\n",
      ">>No reply day ! I mean , week ! | #Happiness\n",
      "N o   r e p l y   d a y   !   I   m e a n   ,   w e e k   !   |   # H a p p i n e s s\n",
      ">>#notcies #eu Liverpool workers \" miscarriage of justice \" victims http://t.co/KKyxaCu18R\n",
      "# n o t c i e s   # e u   L i v e r p o o l   w o r k e r s   \"   m i s c a r r i a g e   o f   j u s t i c e   \"   v i c t i m s   h t t p\n",
      ">>Having a night in with Misha ( she still needs twitter ) , chinese and movies all night #unilife #nightin\n",
      "H a v i n g   a   n i g h t   i n   w i t h   M i s h a   (   s h e   s t i l l   n e e d s   t w i t t e r   )   ,   c h i n e s e   a n d   m o v i e s   a l l   n i g h t   # u n i l i f e   # n i g h t i n\n",
      ">>Great way to start of the day\n",
      "G r e a t   w a y   t o   s t a r t   o f   t h e   d a y\n",
      ">>I just love freezing rain .\n",
      "I   j u s t   l o v e   f r e e z i n g   r a i n   .\n",
      ">>@HadleyFreeman it could have happened to anyone . Your time will come , critics . Oops , apologies . A typical - loving #Brit .\n",
      "@   i t   c o u l d   h a v e   h a p p e n e d   t o   a n y o n e   .   Y o u r   t i m e   w i l l   c o m e   ,   c r i t i c s   .   O o p s   ,   a p o l o g i e s   .   A   t y p i c a l   -   l o v i n g   # B r i t   .\n",
      ">>Now I remember why I buy books online @WaterstonesMK #servicewithasmile\n",
      "N o w   I   r e m e m b e r   w h y   I   b u y   b o o k s   o n l i n e   @   # s e r v i c e w i t h a s m i l e\n",
      ">>@SR_Duncan @JojoKaliski I love how JoJo has kept it classy ...\n",
      "@   @   I   l o v e   h o w   J o J o   h a s   k e p t   i t   c l a s s y   . . .\n",
      ">>* gasp * The girls are at a rehearsal studio today ? You don't say ... Hmmm wonder what on earth for ? : frog_face :: hot_beverage : ️ #MTVStars Little Mix\n",
      "*   g a s p   *   T h e   g i r l s   a r e   a t   a   r e h e a r s a l   s t u d i o   t o d a y   ?   Y o u   d o n ' t   s a y   . . .   H m m m   w o n d e r   w h a t   o n   e a r t h   f o r   ?   :   f r o g _ f a c e   : :   h o t _ b e v e r a g e   :   ️   # M T V S t a r s   L i t t l e   M i x\n",
      ">>@DalailamaQuots I don't recall dalai lama talking about busty girls and celebs but apart from that ...\n",
      "@   I   d o n ' t   r e c a l l   d a l a i   l a m a   t a l k i n g   a b o u t   b u s t y   g i r l s   a n d   c e l e b s   b u t   a p a r t   f r o m   t h a t   . . .\n",
      ">>There's a reason I don't tell my parents anything , cause I always get THE most positive responses\n",
      "T h e r e ' s   a   r e a s o n   I   d o n ' t   t e l l   m y   p a r e n t s   a n y t h i n g   ,   c a u s e   I   a l w a y s   g e t   T H E   m o s t   p o s i t i v e   r e s p o n s e s\n",
      ">>Lifes like a bird , its pretty cute until it craps on your head . #Gharwapsi #72HoursOfCrazy #PowerOfFive #lol #AntiConversionLaw\n",
      "L i f e s   l i k e   a   b i r d   ,   i t s   p r e t t y   c u t e   u n t i l   i t   c r a p s   o n   y o u r   h e a d   .   # G h a r w a p s i   # 7 2 H o u r s O f C r a z y   # P o w e r O f F i v e   # l o l   # A n t i C o n v e r s i o n L a w\n",
      ">>I hope we get an ice palace and an invisible car #SPECTRE\n",
      "I   h o p e   w e   g e t   a n   i c e   p a l a c e   a n d   a n   i n v i s i b l e   c a r   # S P E C T R E\n",
      ">>Wow thanks for the good vibes right before my exam , parents .\n",
      "W o w   t h a n k s   f o r   t h e   g o o d   v i b e s   r i g h t   b e f o r e   m y   e x a m   ,   p a r e n t s   .\n",
      ">>I've definitely caught my sisters flu . Feeling a chesty cough brewing and it's the best start to the week . TGIM\n",
      "I ' v e   d e f i n i t e l y   c a u g h t   m y   s i s t e r s   f l u   .   F e e l i n g   a   c h e s t y   c o u g h   b r e w i n g   a n d   i t ' s   t h e   b e s t   s t a r t   t o   t h e   w e e k   .   T G I M\n",
      ">>The worst is when they consider things like \" conservative tribune \" as a credible source of information . Totally not bias .\n",
      "T h e   w o r s t   i s   w h e n   t h e y   c o n s i d e r   t h i n g s   l i k e   \"   c o n s e r v a t i v e   t r i b u n e   \"   a s   a   c r e d i b l e   s o u r c e   o f   i n f o r m a t i o n   .   T o t a l l y   n o t   b i a s   .\n",
      ">>go subscribe @BoutItMen on youtube https://t.co/w0lhyDAexl for good music\n",
      "g o   s u b s c r i b e   @   o n   y o u t u b e   h t t p   f o r   g o o d   m u s i c\n",
      ">>What is the most important thing that you should do today ? - - Breathe . http://t.co/Xp3nNJQMHW\n",
      "W h a t   i s   t h e   m o s t   i m p o r t a n t   t h i n g   t h a t   y o u   s h o u l d   d o   t o d a y   ?   -   -   B r e a t h e   .   h t t p\n",
      ">>Those mini naps I had all night between sneezing , coughing and blowing my nose were super restful . #fluday4\n",
      "T h o s e   m i n i   n a p s   I   h a d   a l l   n i g h t   b e t w e e n   s n e e z i n g   ,   c o u g h i n g   a n d   b l o w i n g   m y   n o s e   w e r e   s u p e r   r e s t f u l   .   # f l u d a y 4\n",
      ">>@ipathak25 @BJPDelhiState good step by BjP goons\n",
      "@   @   g o o d   s t e p   b y   B j P   g o o n s\n",
      ">>@FoxNews Hey #Russia ! MAKE MONEY #TWEETING YET ? | | http://t.co/Q2WB7riAvK <- This #FOXNews Clip explains how | | https://t.co/cpJZkEgNe7\n",
      "@   H e y   # R u s s i a   !   M A K E   M O N E Y   # T W E E T I N G   Y E T   ?   |   |   h t t p   < -   T h i s   # F O X N e w s   C l i p   e x p l a i n s   h o w   |   |   h t t p\n",
      ">>I cant even watch anime in japan ...\n",
      "I   c a n t   e v e n   w a t c h   a n i m e   i n   j a p a n   . . .\n",
      ">>Powerful picture taken by a #protester the #night before #christmas . The is real . http://t.co/EVdeheqpsj\n",
      "P o w e r f u l   p i c t u r e   t a k e n   b y   a   # p r o t e s t e r   t h e   # n i g h t   b e f o r e   # c h r i s t m a s   .   T h e   i s   r e a l   .   h t t p\n",
      ">>I was going 2 post my opinions on #obama #cuba but i will save it , till after his #speech . #peacemaker\n",
      "I   w a s   g o i n g   2   p o s t   m y   o p i n i o n s   o n   # o b a m a   # c u b a   b u t   i   w i l l   s a v e   i t   ,   t i l l   a f t e r   h i s   # s p e e c h   .   # p e a c e m a k e r\n",
      ">>BBC News - ' Skywalker ' passport cancelled because it was ' frivolous ' #trademarks http://t.co/chTEamOZcC\n",
      "B B C   N e w s   -   '   S k y w a l k e r   '   p a s s p o r t   c a n c e l l e d   b e c a u s e   i t   w a s   '   f r i v o l o u s   '   # t r a d e m a r k s   h t t p\n",
      ">>you're so sweet\n",
      "y o u ' r e   s o   s w e e t\n",
      ">>#bibotour & #champwithin founder @traciscampbell on @WCIU_YouAndMe w / dancers Ricky & Gracey #chicago #women\n",
      "# b i b o t o u r   &   # c h a m p w i t h i n   f o u n d e r   @   o n   @   w   /   d a n c e r s   R i c k y   &   G r a c e y   # c h i c a g o   # w o m e n\n",
      ">>Someone had a great sense of humor #TheyGaveMeTCUColorsForChristmas http://t.co/kUNO4OrUPA\n",
      "S o m e o n e   h a d   a   g r e a t   s e n s e   o f   h u m o r   # T h e y G a v e M e T C U C o l o r s F o r C h r i s t m a s   h t t p\n",
      ">>@ceyland89978315 All # U Can Do Beautiful Is Your #BEST Winning Is Important Taking Part & Having Fun Is What Matter's :-)\n",
      "@   A l l   #   U   C a n   D o   B e a u t i f u l   I s   Y o u r   # B E S T   W i n n i n g   I s   I m p o r t a n t   T a k i n g   P a r t   &   H a v i n g   F u n   I s   W h a t   M a t t e r ' s   : - )\n",
      ">>not speaking he do not like comedy , it was spoken jokingly of his assassination that is funny guys and gals ... ( 2/2 )\n",
      "n o t   s p e a k i n g   h e   d o   n o t   l i k e   c o m e d y   ,   i t   w a s   s p o k e n   j o k i n g l y   o f   h i s   a s s a s s i n a t i o n   t h a t   i s   f u n n y   g u y s   a n d   g a l s   . . .   (   2 / 2   )\n",
      ">>#Law & OrderSVU where rappers go to act\n",
      "# L a w   &   O r d e r S V U   w h e r e   r a p p e r s   g o   t o   a c t\n",
      ">>AAP said will declare AK candidate in last list but declared it before.This issue affecting India's GDP is termed as U-Turn by BJP #AK4Delhi\n",
      "A A P   s a i d   w i l l   d e c l a r e   A K   c a n d i d a t e   i n   l a s t   l i s t   b u t   d e c l a r e d   i t   b e f o r e . T h i s   i s s u e   a f f e c t i n g   I n d i a ' s   G D P   i s   t e r m e d   a s   U - T u r n   b y   B J P   # A K 4 D e l h i\n",
      ">>Haha love when I accidentally spray perfume in my eye : face_with_ok_gesture :\n",
      "H a h a   l o v e   w h e n   I   a c c i d e n t a l l y   s p r a y   p e r f u m e   i n   m y   e y e   :   f a c e _ w i t h _ o k _ g e s t u r e   :\n",
      "\n",
      "[[1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [1, 0, 0, 0, 0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ir2class(k):\n",
    "    onehot=[[1,0,0,0,0],[0,1,0,0,0],[0,0,1,0,0],[0,0,0,1,0],[0,0,0,0,1]]\n",
    "    return onehot[['0','1','2',\"3\",\"4\"].index(k)]\n",
    "\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "X_test=[]\n",
    "y_test=[]\n",
    "for i,t in enumerate(tasks):\n",
    "    X_train.append([text2seq(toks[i],t) for t,c in data_train[i]])\n",
    "    if t==\"taskA\":\n",
    "        y_train.append([int(c) for t,c in data_train[i]])\n",
    "    else:\n",
    "        y_train.append([ir2class(c) for t,c in data_train[i]])\n",
    "    X_test.append([text2seq(toks[i],t) for t,c in data_test[i]])\n",
    "    if t==\"taskA\":\n",
    "        y_test.append([int(c) for t,c in data_test[i]])\n",
    "    else:\n",
    "        y_test.append([ir2class(c) for t,c in data_test[i]])\n",
    "        \n",
    "\n",
    "    print(\"Example train in\",t)\n",
    "    #print(\"\\n\".join([\" \".join([str(w) for w in S]) for S in X_train[i][:5]]))\n",
    "    original=[t for t,c in data_train[i][:50]]\n",
    "    tokenized=[\" \".join([maps_[i][w] for w in S if w]) for S in X_train[i][:50]]\n",
    "        \n",
    "    print(\"\\n>>\".join([\"{0}\\n{1}\".format(a,b) for a,b in zip(original,tokenized)]))\n",
    "    print()\n",
    "    print([c for c in y_train[i][:5]])\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example train\n",
      "F i o r i n a   b l a s t s   C l i n t o n   ,   a s k s   w h y   T r u m p   i s   M I A   -   R e p u b l i c a n   h o p e f u l   C a r l y   F i o r i n a   s a i d   M o n d a y   t h a t   s h e   f e e l s   l i k e   t   . . .   h t t p\n",
      "@   I ' m   d o w n   f o r   t h a t   o b v i o u s l y   ,   b u t   d o e s   N i n t e n d o   w a n t   t h a t   ?   T h e   w o r l d   m a y   n e v e r   k n o w\n",
      "@   T r u e   ,   r i g h t   n o w   t h e y   h a v e   t h e   b e s t   s q u a d   .   B u t   i f   M i l a n   c a n   p u l l   a   f e w   t r a n s f e r s   r i g h t   i n   J a n u a r y   ,   t h e r e ' s   h o p e   f o r   t o p   3   .\n",
      "# E M A   # M A S S I V E A C T I O N   t h i s   S u n d a y   a t   1 2   N o o n   i n   V e n i c e   B e a c h   a t   R o s e   A v e   &   O c e a n   F r o n t   W a l k   o n   t h e   s a n d   !   F r e e   . . .   h t t p\n",
      "f u l l   r e s u l t s   f r o m   E l d o r a   S p e e d w a y   !   f i n i s h e d   2 n d   w i t h   t h e   N R A   !   f b\n",
      "[[1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "X_polarity_train=[]\n",
    "y_polarity_train=[]\n",
    "for i,t in enumerate(tasks):\n",
    "    X_polarity_train.append([text2seq(toks[i],t) for t,c in data_polarity_train])\n",
    "    y_polarity_train.append([c for t,c in data_polarity_train])\n",
    "    \n",
    "print(\"Example train\")\n",
    "print(\"\\n\".join([\" \".join([maps_[i][w] for w in S if w]) for S in X_polarity_train[0][:5]]))\n",
    "print(y_polarity_train[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(task):\n",
    "    \n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    embeedings = Embedding(max_features, embedding_size)(inputs)\n",
    "    dropout=SpatialDropout1D(0.25)(embeedings)\n",
    "    \n",
    "    conv1 = Conv1D(16,\n",
    "                 3,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1)(dropout)\n",
    "    maxpool1 = MaxPooling1D(pool_size=pool_size)(conv1)\n",
    "    conv2 = Conv1D(16,\n",
    "                 5,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1)(dropout)\n",
    "    maxpool2 = MaxPooling1D(pool_size=pool_size)(conv2)\n",
    "    conv3= Conv1D(16,\n",
    "                 7,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1)(dropout)\n",
    "    maxpool3= MaxPooling1D(pool_size=pool_size)(conv3)\n",
    "    conv4= Conv1D(16,\n",
    "                 9,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1)(dropout)\n",
    "    maxpool4=MaxPooling1D(pool_size=pool_size)(conv4)\n",
    "    \n",
    "    concatenate = Maximum()([maxpool1,maxpool2,maxpool3,maxpool4])\n",
    "    bidirectional =  Bidirectional(LSTM(lstm_output_size))(concatenate)\n",
    "    if task==\"taskA\":\n",
    "        irony=Dense(1, activation='sigmoid')(bidirectional)\n",
    "    if task==\"taskB\":\n",
    "        irony=Dense(5, activation='softmax')(bidirectional)\n",
    "    \n",
    "    polarity=Dense(3, activation='softmax')(bidirectional)\n",
    "    model_irony = Model(inputs=inputs, outputs=irony)\n",
    "    model_polarity = Model(inputs=inputs, outputs=polarity)\n",
    "    return model_polarity, model_irony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model task taskA\n",
      "Creating model task taskB\n"
     ]
    }
   ],
   "source": [
    "models=[]\n",
    "for i,t in enumerate(tasks):\n",
    "    print(\"Creating model task\",t)\n",
    "    models.append(build_model(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 80)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 80, 32)       3200        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 80, 32)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 80, 16)       1552        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 80, 16)       2576        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 80, 16)       3600        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 80, 16)       4624        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 20, 16)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 20, 16)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 20, 16)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 20, 16)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "maximum_1 (Maximum)             (None, 20, 16)       0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 64)           12544       maximum_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            195         bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 28,291\n",
      "Trainable params: 28,291\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 41173 samples, validate on 4575 samples\n",
      "Epoch 1/100\n",
      "41173/41173 [==============================] - 42s 1ms/step - loss: 0.9955 - acc: 0.4825 - val_loss: 0.9615 - val_acc: 0.5395\n",
      "Epoch 2/100\n",
      "41173/41173 [==============================] - 37s 896us/step - loss: 0.9502 - acc: 0.5368 - val_loss: 0.9410 - val_acc: 0.5416\n",
      "Epoch 3/100\n",
      "41173/41173 [==============================] - 38s 922us/step - loss: 0.9310 - acc: 0.5446 - val_loss: 0.9212 - val_acc: 0.5565\n",
      "Epoch 4/100\n",
      "41173/41173 [==============================] - 38s 932us/step - loss: 0.9125 - acc: 0.5585 - val_loss: 0.9006 - val_acc: 0.5692\n",
      "Epoch 5/100\n",
      "41173/41173 [==============================] - 39s 958us/step - loss: 0.8965 - acc: 0.5685 - val_loss: 0.8981 - val_acc: 0.5666\n",
      "Epoch 6/100\n",
      "41173/41173 [==============================] - 40s 976us/step - loss: 0.8833 - acc: 0.5753 - val_loss: 0.8882 - val_acc: 0.5711\n",
      "Epoch 7/100\n",
      "41173/41173 [==============================] - 39s 949us/step - loss: 0.8729 - acc: 0.5841 - val_loss: 0.8998 - val_acc: 0.5644\n",
      "Epoch 8/100\n",
      "41173/41173 [==============================] - 39s 945us/step - loss: 0.8644 - acc: 0.5905 - val_loss: 0.8720 - val_acc: 0.5847\n",
      "Epoch 9/100\n",
      "41173/41173 [==============================] - 38s 926us/step - loss: 0.8559 - acc: 0.5949 - val_loss: 0.8685 - val_acc: 0.5860\n",
      "Epoch 10/100\n",
      "41173/41173 [==============================] - 38s 929us/step - loss: 0.8491 - acc: 0.5997 - val_loss: 0.8617 - val_acc: 0.5917\n",
      "Epoch 11/100\n",
      "41173/41173 [==============================] - 38s 922us/step - loss: 0.8401 - acc: 0.6054 - val_loss: 0.8881 - val_acc: 0.5703\n",
      "Epoch 12/100\n",
      "41173/41173 [==============================] - 38s 927us/step - loss: 0.8375 - acc: 0.6058 - val_loss: 0.8634 - val_acc: 0.5899\n",
      "Epoch 13/100\n",
      "41173/41173 [==============================] - 38s 924us/step - loss: 0.8309 - acc: 0.6090 - val_loss: 0.8710 - val_acc: 0.5838\n",
      "Epoch 14/100\n",
      "41173/41173 [==============================] - 39s 954us/step - loss: 0.8267 - acc: 0.6125 - val_loss: 0.8703 - val_acc: 0.5755\n",
      "Epoch 15/100\n",
      "41173/41173 [==============================] - 39s 955us/step - loss: 0.8205 - acc: 0.6155 - val_loss: 0.8858 - val_acc: 0.5670\n",
      "Epoch 16/100\n",
      "41173/41173 [==============================] - 38s 923us/step - loss: 0.8192 - acc: 0.6186 - val_loss: 0.8719 - val_acc: 0.5830\n",
      "Epoch 17/100\n",
      "41173/41173 [==============================] - 38s 930us/step - loss: 0.8139 - acc: 0.6199 - val_loss: 0.8637 - val_acc: 0.5972\n",
      "Epoch 18/100\n",
      "41173/41173 [==============================] - 38s 928us/step - loss: 0.8144 - acc: 0.6182 - val_loss: 0.8658 - val_acc: 0.5847\n",
      "Epoch 19/100\n",
      "41173/41173 [==============================] - 39s 945us/step - loss: 0.8125 - acc: 0.6207 - val_loss: 0.8542 - val_acc: 0.5952\n",
      "Epoch 20/100\n",
      "41173/41173 [==============================] - 39s 948us/step - loss: 0.8068 - acc: 0.6203 - val_loss: 0.8529 - val_acc: 0.5965\n",
      "Epoch 21/100\n",
      "41173/41173 [==============================] - 39s 938us/step - loss: 0.8033 - acc: 0.6266 - val_loss: 0.8507 - val_acc: 0.5952\n",
      "Epoch 22/100\n",
      "41173/41173 [==============================] - 39s 936us/step - loss: 0.8023 - acc: 0.6263 - val_loss: 0.8594 - val_acc: 0.5897\n",
      "Epoch 23/100\n",
      "41173/41173 [==============================] - 38s 934us/step - loss: 0.7972 - acc: 0.6283 - val_loss: 0.8537 - val_acc: 0.5996\n",
      "Epoch 24/100\n",
      "41173/41173 [==============================] - 38s 929us/step - loss: 0.7962 - acc: 0.6299 - val_loss: 0.8457 - val_acc: 0.6031\n",
      "Epoch 25/100\n",
      "41173/41173 [==============================] - 39s 942us/step - loss: 0.7941 - acc: 0.6312 - val_loss: 0.8594 - val_acc: 0.5926\n",
      "Epoch 26/100\n",
      "41173/41173 [==============================] - 40s 980us/step - loss: 0.7910 - acc: 0.6326 - val_loss: 0.8416 - val_acc: 0.6061\n",
      "Epoch 27/100\n",
      "41173/41173 [==============================] - 39s 954us/step - loss: 0.7931 - acc: 0.6305 - val_loss: 0.8485 - val_acc: 0.5961\n",
      "Epoch 28/100\n",
      "41173/41173 [==============================] - 41s 999us/step - loss: 0.7905 - acc: 0.6327 - val_loss: 0.8497 - val_acc: 0.6000\n",
      "Epoch 29/100\n",
      "41173/41173 [==============================] - 41s 990us/step - loss: 0.7889 - acc: 0.6337 - val_loss: 0.8412 - val_acc: 0.6024\n",
      "Epoch 30/100\n",
      "41173/41173 [==============================] - 43s 1ms/step - loss: 0.7845 - acc: 0.6368 - val_loss: 0.8519 - val_acc: 0.5945\n",
      "Epoch 31/100\n",
      "41173/41173 [==============================] - 41s 1ms/step - loss: 0.7821 - acc: 0.6366 - val_loss: 0.8496 - val_acc: 0.5985\n",
      "Epoch 32/100\n",
      "41173/41173 [==============================] - 41s 1ms/step - loss: 0.7840 - acc: 0.6376 - val_loss: 0.8556 - val_acc: 0.5969\n",
      "Epoch 33/100\n",
      "41173/41173 [==============================] - 41s 1ms/step - loss: 0.7794 - acc: 0.6377 - val_loss: 0.8469 - val_acc: 0.5921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100\n",
      "41173/41173 [==============================] - 41s 1ms/step - loss: 0.7823 - acc: 0.6367 - val_loss: 0.8429 - val_acc: 0.6013\n",
      "Epoch 35/100\n",
      "41173/41173 [==============================] - 41s 1ms/step - loss: 0.7763 - acc: 0.6408 - val_loss: 0.8469 - val_acc: 0.6017\n",
      "Epoch 36/100\n",
      "41173/41173 [==============================] - 41s 1ms/step - loss: 0.7754 - acc: 0.6396 - val_loss: 0.8433 - val_acc: 0.6057\n",
      "Epoch 37/100\n",
      "41173/41173 [==============================] - 41s 1000us/step - loss: 0.7745 - acc: 0.6433 - val_loss: 0.8482 - val_acc: 0.6046\n",
      "Epoch 38/100\n",
      "41173/41173 [==============================] - 41s 1ms/step - loss: 0.7755 - acc: 0.6412 - val_loss: 0.8398 - val_acc: 0.6013\n",
      "Epoch 39/100\n",
      "41173/41173 [==============================] - 41s 995us/step - loss: 0.7728 - acc: 0.6424 - val_loss: 0.8488 - val_acc: 0.5967\n",
      "Epoch 40/100\n",
      "41173/41173 [==============================] - 42s 1ms/step - loss: 0.7695 - acc: 0.6455 - val_loss: 0.8380 - val_acc: 0.6120\n",
      "Epoch 41/100\n",
      "41173/41173 [==============================] - 43s 1ms/step - loss: 0.7712 - acc: 0.6410 - val_loss: 0.8427 - val_acc: 0.6028\n",
      "Epoch 42/100\n",
      "41173/41173 [==============================] - 41s 986us/step - loss: 0.7705 - acc: 0.6437 - val_loss: 0.8485 - val_acc: 0.6028\n",
      "Epoch 43/100\n",
      "41173/41173 [==============================] - 40s 970us/step - loss: 0.7663 - acc: 0.6451 - val_loss: 0.8478 - val_acc: 0.5954\n",
      "Epoch 44/100\n",
      "41173/41173 [==============================] - 40s 971us/step - loss: 0.7696 - acc: 0.6464 - val_loss: 0.8396 - val_acc: 0.6024\n",
      "Epoch 45/100\n",
      "41173/41173 [==============================] - 40s 975us/step - loss: 0.7626 - acc: 0.6488 - val_loss: 0.8464 - val_acc: 0.6009\n",
      "Epoch 46/100\n",
      "41173/41173 [==============================] - 40s 969us/step - loss: 0.7667 - acc: 0.6438 - val_loss: 0.8366 - val_acc: 0.6020\n",
      "Epoch 47/100\n",
      "41173/41173 [==============================] - 41s 985us/step - loss: 0.7616 - acc: 0.6515 - val_loss: 0.8425 - val_acc: 0.6007\n",
      "Epoch 48/100\n",
      "41173/41173 [==============================] - 41s 997us/step - loss: 0.7635 - acc: 0.6489 - val_loss: 0.8477 - val_acc: 0.6059\n",
      "Epoch 49/100\n",
      "41173/41173 [==============================] - 41s 1ms/step - loss: 0.7615 - acc: 0.6490 - val_loss: 0.8820 - val_acc: 0.5869\n",
      "Epoch 50/100\n",
      "41173/41173 [==============================] - 41s 1ms/step - loss: 0.7621 - acc: 0.6487 - val_loss: 0.8611 - val_acc: 0.5849\n",
      "Epoch 51/100\n",
      "41173/41173 [==============================] - 40s 983us/step - loss: 0.7587 - acc: 0.6512 - val_loss: 0.8541 - val_acc: 0.5937\n",
      "Epoch 52/100\n",
      "41173/41173 [==============================] - 44s 1ms/step - loss: 0.7574 - acc: 0.6522 - val_loss: 0.8573 - val_acc: 0.5934\n",
      "Epoch 53/100\n",
      "41173/41173 [==============================] - 42s 1ms/step - loss: 0.7585 - acc: 0.6504 - val_loss: 0.8843 - val_acc: 0.5777\n",
      "Epoch 54/100\n",
      "41173/41173 [==============================] - 41s 1ms/step - loss: 0.7566 - acc: 0.6512 - val_loss: 0.8733 - val_acc: 0.5856\n",
      "Epoch 55/100\n",
      "41173/41173 [==============================] - 39s 956us/step - loss: 0.7577 - acc: 0.6510 - val_loss: 0.8681 - val_acc: 0.5873\n",
      "Epoch 56/100\n",
      "41173/41173 [==============================] - 39s 949us/step - loss: 0.7567 - acc: 0.6510 - val_loss: 0.8584 - val_acc: 0.5956\n",
      "Epoch 57/100\n",
      "41173/41173 [==============================] - 38s 926us/step - loss: 0.7548 - acc: 0.6527 - val_loss: 0.8661 - val_acc: 0.5851\n",
      "Epoch 58/100\n",
      "41173/41173 [==============================] - 40s 979us/step - loss: 0.7531 - acc: 0.6541 - val_loss: 0.8551 - val_acc: 0.5954\n",
      "Epoch 59/100\n",
      "41173/41173 [==============================] - 40s 968us/step - loss: 0.7499 - acc: 0.6544 - val_loss: 0.8664 - val_acc: 0.5908\n",
      "Epoch 60/100\n",
      "41173/41173 [==============================] - 39s 957us/step - loss: 0.7523 - acc: 0.6541 - val_loss: 0.8553 - val_acc: 0.6024\n",
      "Epoch 61/100\n",
      "41173/41173 [==============================] - 40s 972us/step - loss: 0.7553 - acc: 0.6519 - val_loss: 0.8646 - val_acc: 0.5917\n",
      "Epoch 62/100\n",
      "41173/41173 [==============================] - 45s 1ms/step - loss: 0.7517 - acc: 0.6569 - val_loss: 0.8686 - val_acc: 0.5906\n",
      "Epoch 63/100\n",
      "41173/41173 [==============================] - 42s 1ms/step - loss: 0.7508 - acc: 0.6548 - val_loss: 0.8497 - val_acc: 0.6050\n",
      "Epoch 64/100\n",
      "41173/41173 [==============================] - 40s 982us/step - loss: 0.7471 - acc: 0.6576 - val_loss: 0.8600 - val_acc: 0.5908\n",
      "Epoch 65/100\n",
      "41173/41173 [==============================] - 39s 947us/step - loss: 0.7478 - acc: 0.6575 - val_loss: 0.8436 - val_acc: 0.6107\n",
      "Epoch 66/100\n",
      "41173/41173 [==============================] - 41s 1ms/step - loss: 0.7440 - acc: 0.6569 - val_loss: 0.8571 - val_acc: 0.5998\n",
      "Epoch 67/100\n",
      "41173/41173 [==============================] - 40s 967us/step - loss: 0.7437 - acc: 0.6582 - val_loss: 0.8815 - val_acc: 0.5840\n",
      "Epoch 68/100\n",
      "41173/41173 [==============================] - 39s 949us/step - loss: 0.7428 - acc: 0.6579 - val_loss: 0.8497 - val_acc: 0.6028\n",
      "Epoch 69/100\n",
      "41173/41173 [==============================] - 40s 967us/step - loss: 0.7427 - acc: 0.6591 - val_loss: 0.8586 - val_acc: 0.5965\n",
      "Epoch 70/100\n",
      "41173/41173 [==============================] - 41s 996us/step - loss: 0.7459 - acc: 0.6585 - val_loss: 0.8545 - val_acc: 0.6094\n",
      "Epoch 71/100\n",
      "41173/41173 [==============================] - 40s 961us/step - loss: 0.7445 - acc: 0.6576 - val_loss: 0.8610 - val_acc: 0.6004\n",
      "Epoch 72/100\n",
      "41173/41173 [==============================] - 42s 1ms/step - loss: 0.7432 - acc: 0.6583 - val_loss: 0.8665 - val_acc: 0.5838\n",
      "Epoch 73/100\n",
      "41173/41173 [==============================] - 43s 1ms/step - loss: 0.7408 - acc: 0.6620 - val_loss: 0.8674 - val_acc: 0.5991\n",
      "Epoch 74/100\n",
      "41173/41173 [==============================] - 44s 1ms/step - loss: 0.7407 - acc: 0.6580 - val_loss: 0.8607 - val_acc: 0.6007\n",
      "Epoch 75/100\n",
      "41173/41173 [==============================] - 44s 1ms/step - loss: 0.7403 - acc: 0.6601 - val_loss: 0.8653 - val_acc: 0.6011\n",
      "Epoch 76/100\n",
      "41173/41173 [==============================] - 43s 1ms/step - loss: 0.7378 - acc: 0.6621 - val_loss: 0.8640 - val_acc: 0.5952\n",
      "Epoch 77/100\n",
      "41173/41173 [==============================] - 42s 1ms/step - loss: 0.7351 - acc: 0.6644 - val_loss: 0.8704 - val_acc: 0.6015\n",
      "Epoch 78/100\n",
      "41173/41173 [==============================] - 41s 1ms/step - loss: 0.7396 - acc: 0.6617 - val_loss: 0.8596 - val_acc: 0.5983\n",
      "Epoch 79/100\n",
      "41173/41173 [==============================] - 42s 1ms/step - loss: 0.7378 - acc: 0.6610 - val_loss: 0.8720 - val_acc: 0.5889\n",
      "Epoch 80/100\n",
      "41173/41173 [==============================] - 41s 1000us/step - loss: 0.7357 - acc: 0.6641 - val_loss: 0.8798 - val_acc: 0.5875\n",
      "Epoch 81/100\n",
      "41173/41173 [==============================] - 42s 1ms/step - loss: 0.7340 - acc: 0.6609 - val_loss: 0.8628 - val_acc: 0.5967\n",
      "Epoch 82/100\n",
      "41173/41173 [==============================] - 41s 992us/step - loss: 0.7377 - acc: 0.6619 - val_loss: 0.8737 - val_acc: 0.5969\n",
      "Epoch 83/100\n",
      "41173/41173 [==============================] - 38s 927us/step - loss: 0.7330 - acc: 0.6634 - val_loss: 0.8676 - val_acc: 0.6024\n",
      "Epoch 84/100\n",
      "41173/41173 [==============================] - 39s 947us/step - loss: 0.7325 - acc: 0.6654 - val_loss: 0.8810 - val_acc: 0.5974\n",
      "Epoch 85/100\n",
      "41173/41173 [==============================] - 39s 941us/step - loss: 0.7302 - acc: 0.6670 - val_loss: 0.8788 - val_acc: 0.5976\n",
      "Epoch 86/100\n",
      "41173/41173 [==============================] - 39s 945us/step - loss: 0.7306 - acc: 0.6681 - val_loss: 0.8631 - val_acc: 0.6063\n",
      "Epoch 87/100\n",
      "41173/41173 [==============================] - 38s 929us/step - loss: 0.7330 - acc: 0.6652 - val_loss: 0.8668 - val_acc: 0.6083\n",
      "Epoch 88/100\n",
      "41173/41173 [==============================] - 38s 920us/step - loss: 0.7311 - acc: 0.6650 - val_loss: 0.8700 - val_acc: 0.6011\n",
      "Epoch 89/100\n",
      "41173/41173 [==============================] - 39s 942us/step - loss: 0.7289 - acc: 0.6667 - val_loss: 0.8625 - val_acc: 0.6046\n",
      "Epoch 90/100\n",
      "41173/41173 [==============================] - 39s 950us/step - loss: 0.7283 - acc: 0.6670 - val_loss: 0.8668 - val_acc: 0.5987\n",
      "Epoch 91/100\n",
      "41173/41173 [==============================] - 39s 936us/step - loss: 0.7299 - acc: 0.6665 - val_loss: 0.8686 - val_acc: 0.6013\n",
      "Epoch 92/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41173/41173 [==============================] - 39s 948us/step - loss: 0.7307 - acc: 0.6655 - val_loss: 0.8728 - val_acc: 0.5943\n",
      "Epoch 93/100\n",
      "41173/41173 [==============================] - 38s 923us/step - loss: 0.7293 - acc: 0.6644 - val_loss: 0.8655 - val_acc: 0.6050\n",
      "Epoch 94/100\n",
      "41173/41173 [==============================] - 38s 923us/step - loss: 0.7270 - acc: 0.6667 - val_loss: 0.8841 - val_acc: 0.5867\n",
      "Epoch 95/100\n",
      "41173/41173 [==============================] - 41s 992us/step - loss: 0.7264 - acc: 0.6642 - val_loss: 0.8837 - val_acc: 0.5948\n",
      "Epoch 96/100\n",
      "41173/41173 [==============================] - 39s 936us/step - loss: 0.7257 - acc: 0.6685 - val_loss: 0.8641 - val_acc: 0.5991\n",
      "Epoch 97/100\n",
      "41173/41173 [==============================] - 39s 945us/step - loss: 0.7250 - acc: 0.6684 - val_loss: 0.8760 - val_acc: 0.6002\n",
      "Epoch 98/100\n",
      "41173/41173 [==============================] - 40s 967us/step - loss: 0.7256 - acc: 0.6675 - val_loss: 0.8814 - val_acc: 0.5954\n",
      "Epoch 99/100\n",
      "41173/41173 [==============================] - 41s 989us/step - loss: 0.7272 - acc: 0.6682 - val_loss: 0.8645 - val_acc: 0.6114\n",
      "Epoch 100/100\n",
      "41173/41173 [==============================] - 40s 979us/step - loss: 0.7221 - acc: 0.6710 - val_loss: 0.8970 - val_acc: 0.5961\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 80)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 80, 32)       3200        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 80, 32)       0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 80, 16)       1552        spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 80, 16)       2576        spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 80, 16)       3600        spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 80, 16)       4624        spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 20, 16)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 20, 16)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 20, 16)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 20, 16)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "maximum_2 (Maximum)             (None, 20, 16)       0           max_pooling1d_5[0][0]            \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "                                                                 max_pooling1d_7[0][0]            \n",
      "                                                                 max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 64)           12544       maximum_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 3)            195         bidirectional_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 28,291\n",
      "Trainable params: 28,291\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 41173 samples, validate on 4575 samples\n",
      "Epoch 1/100\n",
      "41173/41173 [==============================] - 41s 998us/step - loss: 0.9930 - acc: 0.4880 - val_loss: 0.9508 - val_acc: 0.5486\n",
      "Epoch 2/100\n",
      "41173/41173 [==============================] - 38s 923us/step - loss: 0.9487 - acc: 0.5354 - val_loss: 0.9306 - val_acc: 0.5561\n",
      "Epoch 3/100\n",
      "41173/41173 [==============================] - 38s 923us/step - loss: 0.9267 - acc: 0.5499 - val_loss: 0.9134 - val_acc: 0.5624\n",
      "Epoch 4/100\n",
      "41173/41173 [==============================] - 38s 932us/step - loss: 0.9102 - acc: 0.5589 - val_loss: 0.9126 - val_acc: 0.5626\n",
      "Epoch 5/100\n",
      "41173/41173 [==============================] - 39s 954us/step - loss: 0.8915 - acc: 0.5708 - val_loss: 0.8824 - val_acc: 0.5698\n",
      "Epoch 6/100\n",
      "41173/41173 [==============================] - 39s 935us/step - loss: 0.8796 - acc: 0.5820 - val_loss: 0.8793 - val_acc: 0.5670\n",
      "Epoch 7/100\n",
      "41173/41173 [==============================] - 39s 943us/step - loss: 0.8671 - acc: 0.5856 - val_loss: 0.8750 - val_acc: 0.5744\n",
      "Epoch 8/100\n",
      "41173/41173 [==============================] - 42s 1ms/step - loss: 0.8577 - acc: 0.5945 - val_loss: 0.8662 - val_acc: 0.5845\n",
      "Epoch 9/100\n",
      "41173/41173 [==============================] - 40s 964us/step - loss: 0.8488 - acc: 0.5958 - val_loss: 0.8702 - val_acc: 0.5843\n",
      "Epoch 10/100\n",
      "41173/41173 [==============================] - 38s 927us/step - loss: 0.8422 - acc: 0.6024 - val_loss: 0.9063 - val_acc: 0.5574\n",
      "Epoch 11/100\n",
      "41173/41173 [==============================] - 38s 932us/step - loss: 0.8369 - acc: 0.6054 - val_loss: 0.8618 - val_acc: 0.5858\n",
      "Epoch 12/100\n",
      "41173/41173 [==============================] - 39s 950us/step - loss: 0.8303 - acc: 0.6064 - val_loss: 0.8572 - val_acc: 0.5878\n",
      "Epoch 13/100\n",
      "41173/41173 [==============================] - 38s 923us/step - loss: 0.8279 - acc: 0.6081 - val_loss: 0.8720 - val_acc: 0.5755\n",
      "Epoch 14/100\n",
      "41173/41173 [==============================] - 39s 944us/step - loss: 0.8244 - acc: 0.6113 - val_loss: 0.8648 - val_acc: 0.5875\n",
      "Epoch 15/100\n",
      "41173/41173 [==============================] - 38s 924us/step - loss: 0.8172 - acc: 0.6152 - val_loss: 0.8723 - val_acc: 0.5768\n",
      "Epoch 16/100\n",
      "41173/41173 [==============================] - 38s 920us/step - loss: 0.8168 - acc: 0.6184 - val_loss: 0.8688 - val_acc: 0.5849\n",
      "Epoch 17/100\n",
      "41173/41173 [==============================] - 38s 922us/step - loss: 0.8102 - acc: 0.6176 - val_loss: 0.8495 - val_acc: 0.5908\n",
      "Epoch 18/100\n",
      "41173/41173 [==============================] - 38s 930us/step - loss: 0.8073 - acc: 0.6217 - val_loss: 0.8677 - val_acc: 0.5880\n",
      "Epoch 19/100\n",
      "41173/41173 [==============================] - 39s 939us/step - loss: 0.8062 - acc: 0.6241 - val_loss: 0.8617 - val_acc: 0.5805\n",
      "Epoch 20/100\n",
      "41173/41173 [==============================] - 39s 936us/step - loss: 0.8065 - acc: 0.6212 - val_loss: 0.8553 - val_acc: 0.5823\n",
      "Epoch 21/100\n",
      "41173/41173 [==============================] - 38s 934us/step - loss: 0.7995 - acc: 0.6282 - val_loss: 0.8574 - val_acc: 0.5867\n",
      "Epoch 22/100\n",
      "41173/41173 [==============================] - 38s 933us/step - loss: 0.7986 - acc: 0.6274 - val_loss: 0.8523 - val_acc: 0.5889\n",
      "Epoch 23/100\n",
      "41173/41173 [==============================] - 38s 926us/step - loss: 0.7965 - acc: 0.6292 - val_loss: 0.8467 - val_acc: 0.5928\n",
      "Epoch 24/100\n",
      "41173/41173 [==============================] - 39s 936us/step - loss: 0.7928 - acc: 0.6275 - val_loss: 0.8550 - val_acc: 0.5891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100\n",
      "41173/41173 [==============================] - 41s 1ms/step - loss: 0.7917 - acc: 0.6313 - val_loss: 0.8536 - val_acc: 0.5945\n",
      "Epoch 26/100\n",
      "41173/41173 [==============================] - 39s 949us/step - loss: 0.7891 - acc: 0.6327 - val_loss: 0.8479 - val_acc: 0.5963\n",
      "Epoch 27/100\n",
      "41173/41173 [==============================] - 37s 908us/step - loss: 0.7879 - acc: 0.6321 - val_loss: 0.8539 - val_acc: 0.5943\n",
      "Epoch 28/100\n",
      "41173/41173 [==============================] - 37s 909us/step - loss: 0.7853 - acc: 0.6345 - val_loss: 0.8497 - val_acc: 0.5954\n",
      "Epoch 29/100\n",
      "41173/41173 [==============================] - 38s 912us/step - loss: 0.7848 - acc: 0.6348 - val_loss: 0.8520 - val_acc: 0.5902\n",
      "Epoch 30/100\n",
      "41173/41173 [==============================] - 37s 910us/step - loss: 0.7815 - acc: 0.6383 - val_loss: 0.8511 - val_acc: 0.5899\n",
      "Epoch 31/100\n",
      "41173/41173 [==============================] - 37s 909us/step - loss: 0.7810 - acc: 0.6349 - val_loss: 0.8416 - val_acc: 0.5985\n",
      "Epoch 32/100\n",
      "41173/41173 [==============================] - 37s 907us/step - loss: 0.7820 - acc: 0.6369 - val_loss: 0.8529 - val_acc: 0.5904\n",
      "Epoch 33/100\n",
      "41173/41173 [==============================] - 37s 910us/step - loss: 0.7800 - acc: 0.6366 - val_loss: 0.8481 - val_acc: 0.6017\n",
      "Epoch 34/100\n",
      "41173/41173 [==============================] - 37s 909us/step - loss: 0.7798 - acc: 0.6373 - val_loss: 0.8574 - val_acc: 0.5921\n",
      "Epoch 35/100\n",
      "41173/41173 [==============================] - 37s 907us/step - loss: 0.7806 - acc: 0.6368 - val_loss: 0.8433 - val_acc: 0.6031\n",
      "Epoch 36/100\n",
      "41173/41173 [==============================] - 38s 921us/step - loss: 0.7746 - acc: 0.6400 - val_loss: 0.8590 - val_acc: 0.5915\n",
      "Epoch 37/100\n",
      "41173/41173 [==============================] - 41s 1ms/step - loss: 0.7716 - acc: 0.6426 - val_loss: 0.8496 - val_acc: 0.5934\n",
      "Epoch 38/100\n",
      "41173/41173 [==============================] - 37s 905us/step - loss: 0.7753 - acc: 0.6425 - val_loss: 0.8541 - val_acc: 0.5884\n",
      "Epoch 39/100\n",
      "41173/41173 [==============================] - 37s 890us/step - loss: 0.7695 - acc: 0.6445 - val_loss: 0.8879 - val_acc: 0.5849\n",
      "Epoch 40/100\n",
      "41173/41173 [==============================] - 37s 893us/step - loss: 0.7739 - acc: 0.6413 - val_loss: 0.8404 - val_acc: 0.5983\n",
      "Epoch 41/100\n",
      "41173/41173 [==============================] - 37s 895us/step - loss: 0.7704 - acc: 0.6423 - val_loss: 0.8646 - val_acc: 0.5889\n",
      "Epoch 42/100\n",
      "41173/41173 [==============================] - 37s 893us/step - loss: 0.7667 - acc: 0.6451 - val_loss: 0.8467 - val_acc: 0.5897\n",
      "Epoch 43/100\n",
      "41173/41173 [==============================] - 38s 918us/step - loss: 0.7678 - acc: 0.6460 - val_loss: 0.8901 - val_acc: 0.5843\n",
      "Epoch 44/100\n",
      "41173/41173 [==============================] - 38s 932us/step - loss: 0.7694 - acc: 0.6421 - val_loss: 0.8746 - val_acc: 0.5902\n",
      "Epoch 45/100\n",
      "41173/41173 [==============================] - 38s 924us/step - loss: 0.7638 - acc: 0.6473 - val_loss: 0.8532 - val_acc: 0.5937\n",
      "Epoch 46/100\n",
      "41173/41173 [==============================] - 38s 915us/step - loss: 0.7636 - acc: 0.6441 - val_loss: 0.8457 - val_acc: 0.6031\n",
      "Epoch 47/100\n",
      "41173/41173 [==============================] - 38s 924us/step - loss: 0.7650 - acc: 0.6454 - val_loss: 0.8661 - val_acc: 0.5797\n",
      "Epoch 48/100\n",
      "41173/41173 [==============================] - 39s 946us/step - loss: 0.7628 - acc: 0.6493 - val_loss: 0.8468 - val_acc: 0.6026\n",
      "Epoch 49/100\n",
      "41173/41173 [==============================] - 38s 931us/step - loss: 0.7619 - acc: 0.6454 - val_loss: 0.8573 - val_acc: 0.5950\n",
      "Epoch 50/100\n",
      "41173/41173 [==============================] - 38s 933us/step - loss: 0.7623 - acc: 0.6477 - val_loss: 0.8699 - val_acc: 0.5895\n",
      "Epoch 51/100\n",
      "41173/41173 [==============================] - 39s 937us/step - loss: 0.7578 - acc: 0.6481 - val_loss: 0.8483 - val_acc: 0.5945\n",
      "Epoch 52/100\n",
      "41173/41173 [==============================] - 38s 931us/step - loss: 0.7581 - acc: 0.6497 - val_loss: 0.8639 - val_acc: 0.5913\n",
      "Epoch 53/100\n",
      "41173/41173 [==============================] - 38s 914us/step - loss: 0.7582 - acc: 0.6510 - val_loss: 0.8643 - val_acc: 0.5864\n",
      "Epoch 54/100\n",
      "41173/41173 [==============================] - 38s 927us/step - loss: 0.7567 - acc: 0.6487 - val_loss: 0.8473 - val_acc: 0.5985\n",
      "Epoch 55/100\n",
      "41173/41173 [==============================] - 38s 918us/step - loss: 0.7565 - acc: 0.6476 - val_loss: 0.8477 - val_acc: 0.5932\n",
      "Epoch 56/100\n",
      "41173/41173 [==============================] - 38s 934us/step - loss: 0.7532 - acc: 0.6518 - val_loss: 0.8470 - val_acc: 0.5958\n",
      "Epoch 57/100\n",
      "41173/41173 [==============================] - 38s 926us/step - loss: 0.7573 - acc: 0.6482 - val_loss: 0.8395 - val_acc: 0.6048\n",
      "Epoch 58/100\n",
      "41173/41173 [==============================] - 39s 938us/step - loss: 0.7544 - acc: 0.6517 - val_loss: 0.8479 - val_acc: 0.6000\n",
      "Epoch 59/100\n",
      "41173/41173 [==============================] - 39s 956us/step - loss: 0.7551 - acc: 0.6520 - val_loss: 0.8619 - val_acc: 0.5952\n",
      "Epoch 60/100\n",
      "41173/41173 [==============================] - 39s 958us/step - loss: 0.7540 - acc: 0.6503 - val_loss: 0.8735 - val_acc: 0.5810\n",
      "Epoch 61/100\n",
      "41173/41173 [==============================] - 39s 945us/step - loss: 0.7501 - acc: 0.6506 - val_loss: 0.8703 - val_acc: 0.5823\n",
      "Epoch 62/100\n",
      "41173/41173 [==============================] - 39s 952us/step - loss: 0.7511 - acc: 0.6533 - val_loss: 0.8784 - val_acc: 0.5897\n",
      "Epoch 63/100\n",
      "41173/41173 [==============================] - 39s 959us/step - loss: 0.7491 - acc: 0.6556 - val_loss: 0.8588 - val_acc: 0.5904\n",
      "Epoch 64/100\n",
      "41173/41173 [==============================] - 39s 956us/step - loss: 0.7469 - acc: 0.6548 - val_loss: 0.8588 - val_acc: 0.5854\n",
      "Epoch 65/100\n",
      "41173/41173 [==============================] - 38s 933us/step - loss: 0.7486 - acc: 0.6544 - val_loss: 0.8635 - val_acc: 0.5821\n",
      "Epoch 66/100\n",
      "41173/41173 [==============================] - 39s 947us/step - loss: 0.7482 - acc: 0.6553 - val_loss: 0.8636 - val_acc: 0.5867\n",
      "Epoch 67/100\n",
      "41173/41173 [==============================] - 40s 974us/step - loss: 0.7455 - acc: 0.6562 - val_loss: 0.8627 - val_acc: 0.5851\n",
      "Epoch 68/100\n",
      "41173/41173 [==============================] - 38s 932us/step - loss: 0.7472 - acc: 0.6538 - val_loss: 0.8539 - val_acc: 0.5930\n",
      "Epoch 69/100\n",
      "41173/41173 [==============================] - 39s 940us/step - loss: 0.7484 - acc: 0.6530 - val_loss: 0.8500 - val_acc: 0.6037\n",
      "Epoch 70/100\n",
      "41173/41173 [==============================] - 39s 953us/step - loss: 0.7455 - acc: 0.6566 - val_loss: 0.8556 - val_acc: 0.5954\n",
      "Epoch 71/100\n",
      "41173/41173 [==============================] - 39s 942us/step - loss: 0.7434 - acc: 0.6567 - val_loss: 0.8564 - val_acc: 0.5908\n",
      "Epoch 72/100\n",
      "41173/41173 [==============================] - 40s 965us/step - loss: 0.7413 - acc: 0.6564 - val_loss: 0.8624 - val_acc: 0.5974\n",
      "Epoch 73/100\n",
      "41173/41173 [==============================] - 39s 951us/step - loss: 0.7419 - acc: 0.6568 - val_loss: 0.8583 - val_acc: 0.5965\n",
      "Epoch 74/100\n",
      "41173/41173 [==============================] - 40s 975us/step - loss: 0.7411 - acc: 0.6564 - val_loss: 0.8562 - val_acc: 0.5952\n",
      "Epoch 75/100\n",
      "41173/41173 [==============================] - 39s 947us/step - loss: 0.7436 - acc: 0.6563 - val_loss: 0.8799 - val_acc: 0.5775\n",
      "Epoch 76/100\n",
      "41173/41173 [==============================] - 40s 970us/step - loss: 0.7399 - acc: 0.6575 - val_loss: 0.8674 - val_acc: 0.5823\n",
      "Epoch 77/100\n",
      "41173/41173 [==============================] - 40s 977us/step - loss: 0.7410 - acc: 0.6588 - val_loss: 0.8669 - val_acc: 0.5840\n",
      "Epoch 78/100\n",
      "41173/41173 [==============================] - 39s 941us/step - loss: 0.7393 - acc: 0.6608 - val_loss: 0.8652 - val_acc: 0.5856\n",
      "Epoch 79/100\n",
      "41173/41173 [==============================] - 39s 940us/step - loss: 0.7392 - acc: 0.6604 - val_loss: 0.8592 - val_acc: 0.5926\n",
      "Epoch 80/100\n",
      "41173/41173 [==============================] - 39s 941us/step - loss: 0.7371 - acc: 0.6624 - val_loss: 0.8627 - val_acc: 0.5908\n",
      "Epoch 81/100\n",
      "41173/41173 [==============================] - 40s 968us/step - loss: 0.7364 - acc: 0.6598 - val_loss: 0.8668 - val_acc: 0.5910\n",
      "Epoch 82/100\n",
      "41173/41173 [==============================] - 41s 996us/step - loss: 0.7331 - acc: 0.6604 - val_loss: 0.8905 - val_acc: 0.5836\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41173/41173 [==============================] - 39s 952us/step - loss: 0.7370 - acc: 0.6603 - val_loss: 0.8780 - val_acc: 0.5825\n",
      "Epoch 84/100\n",
      "41173/41173 [==============================] - 40s 964us/step - loss: 0.7372 - acc: 0.6608 - val_loss: 0.8738 - val_acc: 0.5819\n",
      "Epoch 85/100\n",
      "41173/41173 [==============================] - 39s 955us/step - loss: 0.7322 - acc: 0.6644 - val_loss: 0.9023 - val_acc: 0.5742\n",
      "Epoch 86/100\n",
      "41173/41173 [==============================] - 40s 960us/step - loss: 0.7355 - acc: 0.6621 - val_loss: 0.8617 - val_acc: 0.5991\n",
      "Epoch 87/100\n",
      "41173/41173 [==============================] - 42s 1ms/step - loss: 0.7390 - acc: 0.6583 - val_loss: 0.8697 - val_acc: 0.5910\n",
      "Epoch 88/100\n",
      "41173/41173 [==============================] - 39s 953us/step - loss: 0.7348 - acc: 0.6611 - val_loss: 0.8719 - val_acc: 0.5884\n",
      "Epoch 89/100\n",
      "41173/41173 [==============================] - 39s 959us/step - loss: 0.7287 - acc: 0.6634 - val_loss: 0.8723 - val_acc: 0.5899\n",
      "Epoch 90/100\n",
      "41173/41173 [==============================] - 40s 969us/step - loss: 0.7315 - acc: 0.6636 - val_loss: 0.8639 - val_acc: 0.5913\n",
      "Epoch 91/100\n",
      "41173/41173 [==============================] - 39s 952us/step - loss: 0.7296 - acc: 0.6663 - val_loss: 0.8764 - val_acc: 0.5840\n",
      "Epoch 92/100\n",
      "41173/41173 [==============================] - 40s 965us/step - loss: 0.7342 - acc: 0.6585 - val_loss: 0.8740 - val_acc: 0.5895\n",
      "Epoch 93/100\n",
      "41173/41173 [==============================] - 39s 950us/step - loss: 0.7337 - acc: 0.6625 - val_loss: 0.8757 - val_acc: 0.5991\n",
      "Epoch 94/100\n",
      "41173/41173 [==============================] - 40s 969us/step - loss: 0.7310 - acc: 0.6628 - val_loss: 0.8776 - val_acc: 0.5856\n",
      "Epoch 95/100\n",
      "41173/41173 [==============================] - 39s 954us/step - loss: 0.7278 - acc: 0.6655 - val_loss: 0.8647 - val_acc: 0.5932\n",
      "Epoch 96/100\n",
      "41173/41173 [==============================] - 39s 959us/step - loss: 0.7284 - acc: 0.6666 - val_loss: 0.8637 - val_acc: 0.5932\n",
      "Epoch 97/100\n",
      "41173/41173 [==============================] - 40s 960us/step - loss: 0.7268 - acc: 0.6652 - val_loss: 0.8597 - val_acc: 0.5899\n",
      "Epoch 98/100\n",
      "41173/41173 [==============================] - 39s 939us/step - loss: 0.7294 - acc: 0.6628 - val_loss: 0.8707 - val_acc: 0.5878\n",
      "Epoch 99/100\n",
      "41173/41173 [==============================] - 39s 945us/step - loss: 0.7257 - acc: 0.6635 - val_loss: 0.8644 - val_acc: 0.5928\n",
      "Epoch 100/100\n",
      "41173/41173 [==============================] - 38s 927us/step - loss: 0.7280 - acc: 0.6647 - val_loss: 0.8653 - val_acc: 0.5899\n"
     ]
    }
   ],
   "source": [
    "scores=[]\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "for i,t in enumerate(tasks):\n",
    "    model_pol=models[i][0]\n",
    "    model_pol.summary()\n",
    "    \n",
    "    model_pol.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    X_polarity_train_=sequence.pad_sequences(X_polarity_train[i], maxlen=maxlen)\n",
    "   \n",
    "    model_pol.fit(X_polarity_train_,y_polarity_train[i],\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs_polarity,\n",
    "                  validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating task taskA\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 80)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 80, 32)       3200        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 80, 32)       0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 80, 16)       1552        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 80, 16)       2576        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 80, 16)       3600        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 80, 16)       4624        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 20, 16)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 20, 16)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 20, 16)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 20, 16)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "maximum_1 (Maximum)             (None, 20, 16)       0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 64)           12544       maximum_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            65          bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 28,161\n",
      "Trainable params: 28,161\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 6s 2ms/step - loss: 0.7006 - acc: 0.4968\n",
      "1151/1151 [==============================] - 1s 889us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.55      0.55       577\n",
      "          1       0.54      0.55      0.54       574\n",
      "\n",
      "avg / total       0.55      0.55      0.55      1151\n",
      "\n",
      "Macro f-score: 0.5456111389141128\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 2s 926us/step - loss: 0.6751 - acc: 0.5639\n",
      "1151/1151 [==============================] - 0s 246us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.67      0.62       577\n",
      "          1       0.61      0.51      0.56       574\n",
      "\n",
      "avg / total       0.59      0.59      0.59      1151\n",
      "\n",
      "Macro f-score: 0.5881594151041869\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 3s 1ms/step - loss: 0.6579 - acc: 0.6016\n",
      "1151/1151 [==============================] - 0s 208us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.54      0.59       577\n",
      "          1       0.61      0.70      0.65       574\n",
      "\n",
      "avg / total       0.63      0.62      0.62      1151\n",
      "\n",
      "Macro f-score: 0.620668549905838\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 2s 912us/step - loss: 0.6399 - acc: 0.6206\n",
      "1151/1151 [==============================] - 0s 215us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.56      0.60       577\n",
      "          1       0.61      0.70      0.65       574\n",
      "\n",
      "avg / total       0.63      0.63      0.62      1151\n",
      "\n",
      "Macro f-score: 0.6238154205944111\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 2s 907us/step - loss: 0.6275 - acc: 0.6347\n",
      "1151/1151 [==============================] - 0s 210us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.67      0.65       577\n",
      "          1       0.65      0.62      0.63       574\n",
      "\n",
      "avg / total       0.64      0.64      0.64      1151\n",
      "\n",
      "Macro f-score: 0.6418230158010816\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 3s 965us/step - loss: 0.6113 - acc: 0.6608\n",
      "1151/1151 [==============================] - 0s 217us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.62      0.63       577\n",
      "          1       0.63      0.65      0.64       574\n",
      "\n",
      "avg / total       0.64      0.64      0.64      1151\n",
      "\n",
      "Macro f-score: 0.6350665821179454\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 2s 927us/step - loss: 0.6058 - acc: 0.6694\n",
      "1151/1151 [==============================] - 0s 212us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.62      0.62       577\n",
      "          1       0.62      0.62      0.62       574\n",
      "\n",
      "avg / total       0.62      0.62      0.62      1151\n",
      "\n",
      "Macro f-score: 0.6238042507644568\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 3s 962us/step - loss: 0.5976 - acc: 0.6675\n",
      "1151/1151 [==============================] - 0s 232us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.58      0.61       577\n",
      "          1       0.62      0.68      0.65       574\n",
      "\n",
      "avg / total       0.63      0.63      0.63      1151\n",
      "\n",
      "Macro f-score: 0.628199881230213\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 3s 950us/step - loss: 0.5862 - acc: 0.6742\n",
      "1151/1151 [==============================] - 0s 216us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.56      0.60       577\n",
      "          1       0.61      0.69      0.65       574\n",
      "\n",
      "avg / total       0.63      0.62      0.62      1151\n",
      "\n",
      "Macro f-score: 0.622327538975324\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 2s 919us/step - loss: 0.5723 - acc: 0.6951\n",
      "1151/1151 [==============================] - 0s 206us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.56      0.60       577\n",
      "          1       0.61      0.70      0.65       574\n",
      "\n",
      "avg / total       0.63      0.63      0.63      1151\n",
      "\n",
      "Macro f-score: 0.625561056693741\n",
      "Evaluating task taskB\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 80)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 80, 32)       3200        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 80, 32)       0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 80, 16)       1552        spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 80, 16)       2576        spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 80, 16)       3600        spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 80, 16)       4624        spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 20, 16)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 20, 16)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 20, 16)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 20, 16)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "maximum_2 (Maximum)             (None, 20, 16)       0           max_pooling1d_5[0][0]            \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "                                                                 max_pooling1d_7[0][0]            \n",
      "                                                                 max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 64)           12544       maximum_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 5)            325         bidirectional_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 28,421\n",
      "Trainable params: 28,421\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 5s 2ms/step - loss: 1.3122 - acc: 0.4051\n",
      "1151/1151 [==============================] - 1s 764us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.81      0.63       577\n",
      "          1       0.38      0.22      0.28       417\n",
      "          2       0.00      0.00      0.00        95\n",
      "          3       0.00      0.00      0.00        62\n",
      "\n",
      "avg / total       0.39      0.48      0.42      1151\n",
      "\n",
      "Macro f-score: 0.2270186417513403\n",
      "Epoch 1/1\n",
      " 128/2683 [>.............................] - ETA: 2s - loss: 1.0347 - acc: 0.4609"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2683/2683 [==============================] - 2s 916us/step - loss: 1.0876 - acc: 0.4965\n",
      "1151/1151 [==============================] - 0s 219us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.75      0.63       577\n",
      "          1       0.44      0.38      0.41       417\n",
      "          2       0.00      0.00      0.00        95\n",
      "          3       0.00      0.00      0.00        62\n",
      "\n",
      "avg / total       0.43      0.51      0.47      1151\n",
      "\n",
      "Macro f-score: 0.26057683623473094\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 2s 907us/step - loss: 1.0546 - acc: 0.5252\n",
      "1151/1151 [==============================] - 0s 203us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.59      0.59       577\n",
      "          1       0.45      0.63      0.53       417\n",
      "          2       0.00      0.00      0.00        95\n",
      "          3       0.00      0.00      0.00        62\n",
      "\n",
      "avg / total       0.46      0.52      0.49      1151\n",
      "\n",
      "Macro f-score: 0.2795930448813658\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 2s 918us/step - loss: 1.0293 - acc: 0.5460\n",
      "1151/1151 [==============================] - 0s 218us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.79      0.66       577\n",
      "          1       0.53      0.43      0.48       417\n",
      "          2       0.00      0.00      0.00        95\n",
      "          3       0.00      0.00      0.00        62\n",
      "\n",
      "avg / total       0.47      0.55      0.50      1151\n",
      "\n",
      "Macro f-score: 0.2836998859965799\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 2s 920us/step - loss: 1.0081 - acc: 0.5583\n",
      "1151/1151 [==============================] - 0s 201us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.78      0.67       577\n",
      "          1       0.54      0.48      0.51       417\n",
      "          2       0.00      0.00      0.00        95\n",
      "          3       0.00      0.00      0.00        62\n",
      "\n",
      "avg / total       0.49      0.57      0.52      1151\n",
      "\n",
      "Macro f-score: 0.2941174277100459\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 2s 909us/step - loss: 0.9786 - acc: 0.5807\n",
      "1151/1151 [==============================] - 0s 213us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.72      0.65       577\n",
      "          1       0.51      0.57      0.54       417\n",
      "          2       0.00      0.00      0.00        95\n",
      "          3       0.00      0.00      0.00        62\n",
      "\n",
      "avg / total       0.49      0.56      0.52      1151\n",
      "\n",
      "Macro f-score: 0.29740449050210604\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 3s 942us/step - loss: 0.9627 - acc: 0.5982\n",
      "1151/1151 [==============================] - 0s 206us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.70      0.65       577\n",
      "          1       0.51      0.59      0.55       417\n",
      "          2       0.00      0.00      0.00        95\n",
      "          3       0.00      0.00      0.00        62\n",
      "\n",
      "avg / total       0.49      0.57      0.53      1151\n",
      "\n",
      "Macro f-score: 0.3008081410080261\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 2s 903us/step - loss: 0.9536 - acc: 0.5952\n",
      "1151/1151 [==============================] - 0s 207us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.88      0.69       577\n",
      "          1       0.63      0.40      0.49       417\n",
      "          2       0.00      0.00      0.00        95\n",
      "          3       0.00      0.00      0.00        62\n",
      "\n",
      "avg / total       0.51      0.58      0.52      1151\n",
      "\n",
      "Macro f-score: 0.29530537401381607\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 2s 890us/step - loss: 0.9349 - acc: 0.6131\n",
      "1151/1151 [==============================] - 0s 215us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.68      0.65       577\n",
      "          1       0.52      0.65      0.58       417\n",
      "          2       0.00      0.00      0.00        95\n",
      "          3       0.00      0.00      0.00        62\n",
      "\n",
      "avg / total       0.50      0.58      0.53      1151\n",
      "\n",
      "Macro f-score: 0.3064501899450015\n",
      "Epoch 1/1\n",
      "2683/2683 [==============================] - 2s 888us/step - loss: 0.9077 - acc: 0.6213\n",
      "1151/1151 [==============================] - 0s 223us/step\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.66      0.65       577\n",
      "          1       0.52      0.68      0.59       417\n",
      "          2       0.00      0.00      0.00        95\n",
      "          3       0.00      0.00      0.00        62\n",
      "\n",
      "avg / total       0.51      0.58      0.54      1151\n",
      "\n",
      "Macro f-score: 0.30900242559242125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i,t in enumerate(tasks):\n",
    "    print(\"Evaluating task\",t)\n",
    "    model_ir=models[i][1]\n",
    "\n",
    "    model_ir.summary()\n",
    "    \n",
    "    if t==\"taskA\":\n",
    "        model_ir.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    else:\n",
    "        model_ir.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    X_train_=sequence.pad_sequences(X_train[i], maxlen=maxlen)\n",
    "    X_test_=sequence.pad_sequences(X_test[i], maxlen=maxlen)\n",
    "    \n",
    "\n",
    "    for it in range(epochs_irony):\n",
    "        model_ir.fit(X_train_,y_train[i],\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=1,\n",
    "                  validation_split=0)\n",
    "    \n",
    "    \n",
    "        #score, acc = model.evaluate(X_test_,y_test[i],batch_size=batch_size)\n",
    "    \n",
    "        y_test_ = model_ir.predict(X_test_, batch_size=batch_size, verbose=1)\n",
    "        if t==\"taskA\":\n",
    "            y_test_ = np.round(y_test_)\n",
    "            ori = y_test[i]\n",
    "        else:\n",
    "            y_test_ = np.argmax(y_test_, axis=1)\n",
    "            ori = np.argmax(y_test[i],axis=1)\n",
    "        \n",
    "        print(classification_report(ori, y_test_))\n",
    "        print(\"Macro f-score:\", f1_score(ori, y_test_, average=\"macro\"))\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
