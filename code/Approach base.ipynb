{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os.path\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Bidirectional, Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "max_features = 15000\n",
    "maxlen = 30\n",
    "embedding_size = 128\n",
    "lstm_output_size = 128\n",
    "batch_size = 128\n",
    "epochs_polarity = 0\n",
    "epochs_irony= 10\n",
    "\n",
    "kernel_size = 3\n",
    "filters = 128\n",
    "pool_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos ironía\n",
    "\n",
    "Se cargan los datos de ironía, por cada tarea:\n",
    "\n",
    "* taskA: Clasificador binario\n",
    "* taskB: Clasificador multi-clase (4?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening files\n",
      "../SemEval2018-Task3/infotec_train_dev/SemEval2018-T3-taskA_train.json\n",
      "Size 2683\n",
      "../SemEval2018-Task3/infotec_train_dev/SemEval2018-T3-taskB_train.json\n",
      "Size 2683\n",
      "Opening files\n",
      "../SemEval2018-Task3/infotec_train_dev/SemEval2018-T3-taskA_test.json\n",
      "Size 1151\n",
      "../SemEval2018-Task3/infotec_train_dev/SemEval2018-T3-taskB_test.json\n",
      "Size 1151\n"
     ]
    }
   ],
   "source": [
    "def load_files(files):\n",
    "    json_files=[]\n",
    "    print(\"Opening files\")\n",
    "    for filename in files:\n",
    "        print(filename)\n",
    "        data=[]\n",
    "        for line in codecs.open(filename):\n",
    "            data.append(json.loads(line))\n",
    "        json_files.append(data)\n",
    "        print(\"Size\",len(json_files[-1]))\n",
    "    return json_files\n",
    "\n",
    "\n",
    "tasks=[\"taskA\",\"taskB\"]\n",
    "dirname=\"../SemEval2018-Task3/infotec_train_dev\"\n",
    "basename=\"SemEval2018-T3-{0}_{1}.json\"\n",
    "train_files=[os.path.join(dirname,basename.format(task,'train')) for task in tasks]\n",
    "test_files=[os.path.join(dirname,basename.format(task,'test')) for task in tasks]\n",
    "\n",
    "train_json=load_files(train_files)\n",
    "test_json=load_files(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos de polaridad\n",
    "\n",
    "Cargando datos de polaridad, una sola tarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening files\n",
      "../extras/En.json\n",
      "Size 45748\n"
     ]
    }
   ],
   "source": [
    "train_polarity_json=load_files([\"../extras/En.json\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 2683\n",
      "Size: 1151\n",
      "Size: 2683\n",
      "Size: 1151\n",
      "Size: 45748\n"
     ]
    }
   ],
   "source": [
    "# Configurando datos de ironía\n",
    "\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "nltk_tok=TweetTokenizer(reduce_len=True)\n",
    "\n",
    "data_train=[]\n",
    "data_test=[]\n",
    "for i,t in enumerate(tasks):\n",
    "    text_train=[\" \".join(nltk_tok.tokenize(j['text'])) for j in train_json[i]]\n",
    "    class_train=[j['klass'] for j in train_json[i]]\n",
    "    data_train.append(list(zip(text_train,class_train)))\n",
    "    text_test=[\" \".join(nltk_tok.tokenize(j['text'])) for j in test_json[i]]\n",
    "    class_test=[j['klass'] for j in test_json[i]]\n",
    "    data_test.append(list(zip(text_test,class_test)))\n",
    "\n",
    "\n",
    "for i,t in enumerate(data_train):\n",
    "    print(\"Size:\",len(data_train[i]))\n",
    "    print(\"Size:\",len(data_test[i]))\n",
    "    \n",
    "# Configurando datos de polaridad\n",
    "\n",
    "def pol2class(k):\n",
    "    onehot=[[1,0,0],[0,1,0],[0,0,1]]\n",
    "    return onehot[['neutral','positive','negative'].index(k)]\n",
    "\n",
    "text_polarity_train=[\" \".join(nltk_tok.tokenize(j['text'])) for j in train_polarity_json]\n",
    "class_polarity_train=[pol2class(j['klass']) for j in train_polarity_json]\n",
    "data_polarity_train=list(zip(text_polarity_train,class_polarity_train))\n",
    "\n",
    "print(\"Size:\",len(data_polarity_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "  \n",
    "\n",
    "def normalizeText(tweet):\n",
    "    #tweet = re.sub(r'#(S|s)arcasm|#(I|i)rony','',tweet)\n",
    "    #tweet = re.sub(r'#SARCASM|#IRONY','',tweet)\n",
    "    #tweet = re.sub(r'https?://t\\.co/.(\\w|\\d)+','http://link', tweet) #tweet link\n",
    "    #tweet = re.sub(r'@.\\w*','@',tweet)\n",
    "    #tweet = re.sub(r'#','',tweet)\n",
    "    return tweet\n",
    "\n",
    "def buildTokenizer(tweets):\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_features,lower=True, filters='\\t\\n', split=\" \")\n",
    "    tokenizer.fit_on_texts([\" \".join(nltk_tok.tokenize(t)) for t in tweets])\n",
    "    return tokenizer\n",
    "\n",
    "def text2seq(tok,tweet):\n",
    "    return tok.texts_to_sequences([tweet])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_counts 81045\n",
      "word_docs 81045\n",
      "word_index 81045\n",
      "Top words\n",
      " the (39125) , . (34484) , , (23124) , to (21713) , ! (16267) , in (14167) , a (13942) , on (13384) , i (13378) , and (13253) , of (11426) , for (10306) , is (10266) , ... (8831) , you (8511) , with (7860) , : (7676) , be (7512) , ? (7444) , at (7158) , it (7094) , may (7008) , tomorrow (6982) , \" (6359) , that (5014) , my (4945) , have (4702) , this (4461) , - (4371) , but (4166) , just (4101) , day (3938) , will (3763) , was (3711) , he (3297) , not (3277) , so (3246) , & (3159) , me (3109) , out (3021) , if (2969) , going (2958) , all (2951) , see (2899) , night (2893) , i'm (2833) , from (2775) , are (2767) , ' (2719) , friday (2715) \n",
      "Last words\n",
      " unload (3) , abdul (3) , rode (3) , scares (3) , optimism (3) , adopt (3) , theopen (3) , #coybig (3) , influencing (3) , exposes (3) , theatrical (3) , ceremonial (3) , brewing's (3) , udinese (3) , foamposite (3) , softbank (3) , brah (3) , 6p (3) , mtn (3) , @ranaayyub (3) , whine (3) , beirut (3) , flock (3) , onemk (3) , frances (3) , firmware (3) , sequence (3) , childish (3) , #hillary2016 (3) , alll (3) , divorced (3) , #madeinamerica (3) , remeber (3) , xox (3) , dubstep (3) , claret (3) , nsync (3) , genetic (3) , 7:05 (3) , confront (3) , ali's (3) , 157 (3) , @woodysgamertag (3) , knick (3) , chats (3) , payment (3) , #freedomforkhalistan (3) , designated (3) , utero (3) , @60minutes (3) \n",
      "word_counts 81043\n",
      "word_docs 81043\n",
      "word_index 81043\n",
      "Top words\n",
      " the (39105) , . (34534) , , (23111) , to (21720) , ! (16263) , in (14171) , a (13970) , on (13383) , i (13377) , and (13235) , of (11451) , for (10316) , is (10267) , ... (8830) , you (8524) , with (7859) , : (7671) , be (7515) , ? (7460) , at (7145) , it (7100) , may (7009) , tomorrow (6984) , \" (6360) , that (5015) , my (4940) , have (4721) , this (4470) , - (4374) , but (4171) , just (4093) , day (3946) , will (3762) , was (3703) , he (3298) , not (3271) , so (3244) , & (3155) , me (3103) , out (3025) , if (2979) , going (2963) , all (2944) , see (2900) , night (2894) , i'm (2834) , from (2770) , are (2764) , ' (2715) , friday (2714) \n",
      "Last words\n",
      " brewing's (3) , udinese (3) , foamposite (3) , softbank (3) , brah (3) , 6p (3) , mtn (3) , @ranaayyub (3) , whine (3) , beirut (3) , flock (3) , onemk (3) , frances (3) , firmware (3) , sequence (3) , childish (3) , #hillary2016 (3) , alll (3) , divorced (3) , #madeinamerica (3) , remeber (3) , xox (3) , dubstep (3) , claret (3) , nsync (3) , genetic (3) , 7:05 (3) , confront (3) , ali's (3) , 157 (3) , @woodysgamertag (3) , knick (3) , chats (3) , payment (3) , #freedomforkhalistan (3) , designated (3) , utero (3) , @60minutes (3) , paranormal (3) , long-term (3) , chillen (3) , 9/26 (3) , grabbing (3) , #pgatour (3) , earrings (3) , cuppa (3) , mallorca (3) , #earthquake (3) , cutler (3) , mans (3) \n"
     ]
    }
   ],
   "source": [
    "toks=[]\n",
    "maps_=[]\n",
    "for i,t in enumerate(tasks):\n",
    "    tweets=[t for t,c in data_train[i]]\n",
    "    tweets_=[t for t,c in data_polarity_train]\n",
    "    toks.append(buildTokenizer([normalizeText(t) for t in tweets+tweets_]))\n",
    "    maps_.append({v: k for k, v in toks[-1].word_index.items()})\n",
    "    print(\"word_counts\",len(toks[-1].word_counts))\n",
    "    print(\"word_docs\",len(toks[-1].word_docs))\n",
    "    print(\"word_index\",len(toks[-1].word_index))\n",
    "    print(\"Top words\\n\",\", \".join([\"{0} ({1}) \".format(maps_[-1][f+1],toks[i].word_counts[maps_[-1][f+1]]) for f in range(50)]))\n",
    "    print(\"Last words\\n\",\", \".join([\"{0} ({1}) \".format(maps_[-1][f],toks[i].word_counts[maps_[-1][f]]) for f in range(max_features-50,max_features)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example train in taskA\n",
      "The mouse's first incepted memory was just the sound : BRAAAWWWP !\n",
      "I LOVE not sleeping . It's the best .\n",
      "Religion is unfounded , else , Allah would have saved the kids . . @tariqmushtaqkh @nicpradhan #PeshawarAttack #PakSchoolSiege\n",
      "Love how I came into work at 8 because Charlie said we were busy ... 3 people in 45 minutes , yeah we got this place packed Charlie .\n",
      "Thx for catching on #urock\n",
      "\n",
      "the first memory was just the sound : !\n",
      "i love not sleeping . it's the best .\n",
      "religion is , else , allah would have saved the kids . . #peshawarattack\n",
      "love how i came into work at 8 because charlie said we were busy ... 3 people in 45 minutes , yeah we got this place packed charlie .\n",
      "thx for catching on\n",
      "\n",
      "[0, 1, 1, 1, 1]\n",
      "\n",
      "Example train in taskB\n",
      "Produce all kinds of Creative Designs #like15 | http://t.co/OXeuznMhY8 http://t.co/w4eZ9mObFJ\n",
      "there is only 1 race , HUMAN so i dont look at things by the myth of \" races \" @jtarleta53 @RBRNetwork1\n",
      "@bigbillybmoney oh haha no they ain't : face_with_tears_of_joy :: face_with_tears_of_joy : you ain't even seen em\n",
      "@Only1Neets Probably a Nigerian .\n",
      "#newjob Urgent CONTRACT - Assurance Manager - Private Banking , Singapore - Not Specified , Singapore Specifie ... http://t.co/bASQ2zWos1\n",
      "\n",
      "produce all kinds of creative designs |\n",
      "there is only 1 race , human so i dont look at things by the myth of \" races \" @rbrnetwork1\n",
      "oh haha no they ain't : face_with_tears_of_joy :: face_with_tears_of_joy : you ain't even seen em\n",
      "probably a nigerian .\n",
      "urgent contract - manager - private banking , singapore - not , singapore ...\n",
      "\n",
      "[[1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [1, 0, 0, 0, 0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ir2class(k):\n",
    "    onehot=[[1,0,0,0,0],[0,1,0,0,0],[0,0,1,0,0],[0,0,0,1,0],[0,0,0,0,1]]\n",
    "    return onehot[['0','1','2',\"3\",\"4\"].index(k)]\n",
    "\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "X_test=[]\n",
    "y_test=[]\n",
    "for i,t in enumerate(tasks):\n",
    "    X_train.append([text2seq(toks[i],t) for t,c in data_train[i]])\n",
    "    if t==\"taskA\":\n",
    "        y_train.append([int(c) for t,c in data_train[i]])\n",
    "    else:\n",
    "        y_train.append([ir2class(c) for t,c in data_train[i]])\n",
    "    X_test.append([text2seq(toks[i],t) for t,c in data_test[i]])\n",
    "    if t==\"taskA\":\n",
    "        y_test.append([int(c) for t,c in data_test[i]])\n",
    "    else:\n",
    "        y_test.append([ir2class(c) for t,c in data_test[i]])\n",
    "        \n",
    "\n",
    "    print(\"Example train in\",t)\n",
    "    #print(\"\\n\".join([\" \".join([str(w) for w in S]) for S in X_train[i][:5]]))\n",
    "    print(\"\\n\".join([t for t,c in data_train[i][:5]]))\n",
    "    print()\n",
    "    print(\"\\n\".join([\" \".join([maps_[i][w] for w in S if w]) for S in X_train[i][:5]]))\n",
    "    print()\n",
    "    print([c for c in y_train[i][:5]])\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example train\n",
      "x maths shine , bayern why trump is impression - busy prepared wrong x live monday that she aren't like gen ...\n",
      "i'm down for that they'll , but open card want that ? the really may never know\n",
      "ocean , people now do have the best sister . but if concert can mention a few alas people in january , bryant play for march 3 .\n",
      "this sunday at naruto blast in plans forget at lives arian & 2015 catch walk on the banned ! top ...\n",
      "full pull from felicia finishes ! kurt 2nd with the kart !\n",
      "[[1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "X_polarity_train=[]\n",
    "y_polarity_train=[]\n",
    "for i,t in enumerate(tasks):\n",
    "    X_polarity_train.append([text2seq(toks[i],t) for t,c in data_polarity_train])\n",
    "    y_polarity_train.append([c for t,c in data_polarity_train])\n",
    "    \n",
    "print(\"Example train\")\n",
    "print(\"\\n\".join([\" \".join([maps_[i][w] for w in S if w]) for S in X_polarity_train[0][:5]]))\n",
    "print(y_polarity_train[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(task):\n",
    "    \n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    embeedings = Embedding(max_features, embedding_size)(inputs)\n",
    "    \n",
    "    conv1 = Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1)(embeedings)\n",
    "    maxpool1 = MaxPooling1D(pool_size=pool_size)(conv1)\n",
    "    bidirectional =  LSTM(lstm_output_size,\n",
    "                        activation='tanh', \n",
    "                        recurrent_activation='hard_sigmoid', \n",
    "                        dropout=0.5, \n",
    "                        recurrent_dropout=0.5\n",
    "                        )(maxpool1)\n",
    "    if task==\"taskA\":\n",
    "        irony=Dense(1, activation='sigmoid')(bidirectional)\n",
    "    if task==\"taskB\":\n",
    "        irony=Dense(5, activation='softmax')(bidirectional)\n",
    "    \n",
    "    polarity=Dense(3, activation='softmax')(bidirectional)\n",
    "    model_irony = Model(inputs=inputs, outputs=irony)\n",
    "    model_polarity = Model(inputs=inputs, outputs=polarity)\n",
    "    return model_polarity, model_irony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating task taskA\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 30, 128)           1920000   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 28, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 14, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,101,251\n",
      "Trainable params: 2,101,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 30, 128)           1920000   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 28, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 14, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,100,993\n",
      "Trainable params: 2,100,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 41173 samples, validate on 4575 samples\n",
      "Epoch 1/1\n",
      "41173/41173 [==============================] - 50s 1ms/step - loss: 0.8529 - acc: 0.5947 - val_loss: 0.7375 - val_acc: 0.6695\n",
      "Train on 2414 samples, validate on 269 samples\n",
      "Epoch 1/1\n",
      "2414/2414 [==============================] - 4s 2ms/step - loss: 0.6769 - acc: 0.5833 - val_loss: 0.6712 - val_acc: 0.5799\n",
      "Train on 41173 samples, validate on 4575 samples\n",
      "Epoch 1/1\n",
      "41173/41173 [==============================] - 50s 1ms/step - loss: 0.6557 - acc: 0.7124 - val_loss: 0.7317 - val_acc: 0.6752\n",
      "Train on 2414 samples, validate on 269 samples\n",
      "Epoch 1/1\n",
      "2414/2414 [==============================] - 3s 1ms/step - loss: 0.6452 - acc: 0.6321 - val_loss: 0.6705 - val_acc: 0.5874\n",
      "Train on 41173 samples, validate on 4575 samples\n",
      "Epoch 1/1\n",
      "41173/41173 [==============================] - 50s 1ms/step - loss: 0.5455 - acc: 0.7690 - val_loss: 0.7908 - val_acc: 0.6654\n",
      "Train on 2414 samples, validate on 269 samples\n",
      "Epoch 1/1\n",
      "2414/2414 [==============================] - 3s 1ms/step - loss: 0.6177 - acc: 0.6599 - val_loss: 0.6706 - val_acc: 0.5911\n",
      "Train on 41173 samples, validate on 4575 samples\n",
      "Epoch 1/1\n",
      "41173/41173 [==============================] - 50s 1ms/step - loss: 0.4540 - acc: 0.8149 - val_loss: 0.8757 - val_acc: 0.6645\n",
      "Train on 2414 samples, validate on 269 samples\n",
      "Epoch 1/1\n",
      "2414/2414 [==============================] - 3s 1ms/step - loss: 0.5739 - acc: 0.6831 - val_loss: 0.7078 - val_acc: 0.6022\n",
      "Train on 41173 samples, validate on 4575 samples\n",
      "Epoch 1/1\n",
      "41173/41173 [==============================] - 51s 1ms/step - loss: 0.3677 - acc: 0.8545 - val_loss: 0.9853 - val_acc: 0.6470\n",
      "Train on 2414 samples, validate on 269 samples\n",
      "Epoch 1/1\n",
      "2414/2414 [==============================] - 3s 1ms/step - loss: 0.5248 - acc: 0.7386 - val_loss: 0.7459 - val_acc: 0.5874\n",
      "Train on 41173 samples, validate on 4575 samples\n",
      "Epoch 1/1\n",
      "41173/41173 [==============================] - 50s 1ms/step - loss: 0.2993 - acc: 0.8841 - val_loss: 1.1002 - val_acc: 0.6297\n",
      "Train on 2414 samples, validate on 269 samples\n",
      "Epoch 1/1\n",
      "2414/2414 [==============================] - 3s 1ms/step - loss: 0.4726 - acc: 0.7668 - val_loss: 0.8009 - val_acc: 0.5651\n",
      "Train on 41173 samples, validate on 4575 samples\n",
      "Epoch 1/1\n",
      "41173/41173 [==============================] - 50s 1ms/step - loss: 0.2417 - acc: 0.9089 - val_loss: 1.2739 - val_acc: 0.6297\n",
      "Train on 2414 samples, validate on 269 samples\n",
      "Epoch 1/1\n",
      "2414/2414 [==============================] - 3s 1ms/step - loss: 0.4205 - acc: 0.8003 - val_loss: 0.8727 - val_acc: 0.5651\n",
      "Train on 41173 samples, validate on 4575 samples\n",
      "Epoch 1/1\n",
      "41173/41173 [==============================] - 50s 1ms/step - loss: 0.1947 - acc: 0.9267 - val_loss: 1.4870 - val_acc: 0.6299\n",
      "Train on 2414 samples, validate on 269 samples\n",
      "Epoch 1/1\n",
      "2414/2414 [==============================] - 3s 1ms/step - loss: 0.3662 - acc: 0.8405 - val_loss: 0.9250 - val_acc: 0.5613\n",
      "Train on 41173 samples, validate on 4575 samples\n",
      "Epoch 1/1\n",
      "41173/41173 [==============================] - 54s 1ms/step - loss: 0.1570 - acc: 0.9432 - val_loss: 1.7000 - val_acc: 0.6273\n",
      "Train on 2414 samples, validate on 269 samples\n",
      "Epoch 1/1\n",
      "2414/2414 [==============================] - 3s 1ms/step - loss: 0.3064 - acc: 0.8695 - val_loss: 1.0299 - val_acc: 0.5539\n",
      "Train on 41173 samples, validate on 4575 samples\n",
      "Epoch 1/1\n",
      "32384/41173 [======================>.......] - ETA: 10s - loss: 0.1292 - acc: 0.9535"
     ]
    }
   ],
   "source": [
    "scores=[]\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import numpy as np\n",
    "\n",
    "for i,t in enumerate(tasks[:1]):\n",
    "    print(\"Evaluating task\",t)\n",
    "    model_pol, model_ir = build_model(t)\n",
    "    model_pol.summary()\n",
    "    model_ir.summary()\n",
    "    \n",
    "    model_pol.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model_ir.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    X_polarity_train_=sequence.pad_sequences(X_polarity_train[i], maxlen=maxlen)\n",
    "    \n",
    "    \n",
    "    X_train_=sequence.pad_sequences(X_train[i], maxlen=maxlen)\n",
    "    X_test_=sequence.pad_sequences(X_test[i], maxlen=maxlen)\n",
    "    \n",
    "   \n",
    "    for tmp in range(10):\n",
    "        model_pol.fit(X_polarity_train_,y_polarity_train[i],\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=1,\n",
    "                  validation_split=0.1)\n",
    "   \n",
    "\n",
    "        model_ir.fit(X_train_,y_train[i],\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=1,\n",
    "                  validation_split=0.1)\n",
    "    \n",
    "    \n",
    "    #score, acc = model.evaluate(X_test_,y_test[i],batch_size=batch_size)\n",
    "    \n",
    "    y_test_ = model_ir.predict(X_test_, batch_size=batch_size, verbose=1)\n",
    "    if t==\"taskA\":\n",
    "        y_test_ = np.round(y_test_)\n",
    "    else:\n",
    "        y_test_ = np.argmax(y_test_)\n",
    "        \n",
    "    \n",
    "    print(classification_report(y_test[i], y_test_))\n",
    "    print(\"Macro f-score:\", f1_score(y_test[i], y_test_))\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
