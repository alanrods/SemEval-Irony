{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os.path\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Bidirectional, Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import SpatialDropout1D, Concatenate, Maximum\n",
    "\n",
    "max_features = 100\n",
    "maxlen = 156\n",
    "embedding_size = 32\n",
    "lstm_output_size = 32\n",
    "batch_size = 128\n",
    "epochs_polarity = 40\n",
    "epochs_irony= 40\n",
    "\n",
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos ironía\n",
    "\n",
    "Se cargan los datos de ironía, por cada tarea:\n",
    "\n",
    "* taskA: Clasificador binario\n",
    "* taskB: Clasificador multi-clase (4?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening files\n",
      "../SemEval2018-Task3/infotec_train_dev/SemEval2018-T3-taskA_train.json\n",
      "Size 2683\n",
      "../SemEval2018-Task3/infotec_train_dev/SemEval2018-T3-taskB_train.json\n",
      "Size 2683\n",
      "Opening files\n",
      "../SemEval2018-Task3/infotec_train_dev/SemEval2018-T3-taskA_test.json\n",
      "Size 1151\n",
      "../SemEval2018-Task3/infotec_train_dev/SemEval2018-T3-taskB_test.json\n",
      "Size 1151\n"
     ]
    }
   ],
   "source": [
    "def load_files(files):\n",
    "    json_files=[]\n",
    "    print(\"Opening files\")\n",
    "    for filename in files:\n",
    "        print(filename)\n",
    "        data=[]\n",
    "        for line in codecs.open(filename):\n",
    "            data.append(json.loads(line))\n",
    "        json_files.append(data)\n",
    "        print(\"Size\",len(json_files[-1]))\n",
    "    return json_files\n",
    "\n",
    "\n",
    "tasks=[\"taskA\",\"taskB\"]\n",
    "dirname=\"../SemEval2018-Task3/infotec_train_dev\"\n",
    "basename=\"SemEval2018-T3-{0}_{1}.json\"\n",
    "train_files=[os.path.join(dirname,basename.format(task,'train')) for task in tasks]\n",
    "test_files=[os.path.join(dirname,basename.format(task,'test')) for task in tasks]\n",
    "\n",
    "train_json=load_files(train_files)\n",
    "test_json=load_files(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos de polaridad\n",
    "\n",
    "Cargando datos de polaridad, una sola tarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening files\n",
      "../extras/En.json\n",
      "Size 45748\n"
     ]
    }
   ],
   "source": [
    "train_polarity_json=load_files([\"../extras/En.json\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 2683\n",
      "Size: 1151\n",
      "Size: 2683\n",
      "Size: 1151\n",
      "Size: 45748\n"
     ]
    }
   ],
   "source": [
    "# Configurando datos de ironía\n",
    "\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "nltk_tok=TweetTokenizer(reduce_len=True)\n",
    "\n",
    "data_train=[]\n",
    "data_test=[]\n",
    "for i,t in enumerate(tasks):\n",
    "    text_train=[\" \".join(nltk_tok.tokenize(j['text'])) for j in train_json[i]]\n",
    "    class_train=[j['klass'] for j in train_json[i]]\n",
    "    data_train.append(list(zip(text_train,class_train)))\n",
    "    text_test=[\" \".join(nltk_tok.tokenize(j['text'])) for j in test_json[i]]\n",
    "    class_test=[j['klass'] for j in test_json[i]]\n",
    "    data_test.append(list(zip(text_test,class_test)))\n",
    "\n",
    "\n",
    "for i,t in enumerate(data_train):\n",
    "    print(\"Size:\",len(data_train[i]))\n",
    "    print(\"Size:\",len(data_test[i]))\n",
    "    \n",
    "# Configurando datos de polaridad\n",
    "\n",
    "def pol2class(k):\n",
    "    onehot=[[1,0,0],[0,1,0],[0,0,1]]\n",
    "    return onehot[['neutral','positive','negative'].index(k)]\n",
    "\n",
    "text_polarity_train=[\" \".join(nltk_tok.tokenize(j['text'])) for j in train_polarity_json]\n",
    "class_polarity_train=[pol2class(j['klass']) for j in train_polarity_json]\n",
    "data_polarity_train=list(zip(text_polarity_train,class_polarity_train))\n",
    "\n",
    "print(\"Size:\",len(data_polarity_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "  \n",
    "\n",
    "def normalizeText(tweet):\n",
    "    #tweet = re.sub(r'#(S|s)arcasm|#(I|i)rony','',tweet)\n",
    "    #tweet = re.sub(r'#SARCASM|#IRONY','',tweet)\n",
    "    tweet = re.sub(r'https?://t\\.co/.(\\w|\\d)+','http', tweet) #tweet link\n",
    "    tweet = re.sub(r'fb\\.me/.(\\w|\\d)+','fb', tweet) #tweet link\n",
    "    tweet = re.sub(r'https?://.+','http', tweet) #tweet link\n",
    "    #tweet = re.sub(r'@.\\w*','@',tweet)\n",
    "    #tweet = re.sub(r'#','',tweet)\n",
    "    return tweet\n",
    "\n",
    "def buildTokenizer(tweets):\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=max_features,lower=False, filters='\\t\\n', split=\" \",char_level=True )\n",
    "    tokenizer.fit_on_texts([\" \".join(nltk_tok.tokenize(t)) for t in tweets])\n",
    "    return tokenizer\n",
    "\n",
    "def text2seq(tok,tweet):\n",
    "    return tok.texts_to_sequences([normalizeText(tweet)])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_counts 135\n",
      "word_docs 135\n",
      "word_index 135\n",
      "Top words\n",
      "   (1009790) , e (410768) , t (350042) , a (326811) , o (313591) , n (266978) , i (256924) , r (225927) , s (221057) , h (189295) , l (156551) , d (134555) , u (112173) , y (104607) , m (104043) , g (88165) , c (88139) , w (77363) , p (70882) , . (68693) , f (64999) , b (61752) , k (45592) , v (36452) , S (33038) , I (30219) , T (26785) , ' (25133) , , (23257) , M (22501) , A (22400) , C (21282) , B (19320) , @ (19209) , ! (16269) , D (16146) , 1 (15610) , W (15076) , N (14801) , # (14525) , R (14434) , P (14226) , F (14016) , H (13373) , O (12814) , L (12536) , G (12415) , E (12021) , 2 (11765) , : (10709) \n",
      "Last words\n",
      " > (311) , < (292) , % (272) , = (258) , ] (207) , [ (206) , ~ (200) , ^ (142) , ` (35) , \\ (33) , ️ (25) , } (9) , – (8) , { (7) , £ (5) , \n",
      " (5) , ’ (3) , ☆ (3) , ・ (3) , — (2) , Ｏ (2) , Ｌ (2) , İ (2) , ℃ (2) , 你 (2) , ï (1) , 󾍁 (1) , ° (1) , ง (1) , ว (1) , ย (1) , Ｆ (1) , Ｗ (1) , « (1) , ó (1) , í (1) , ⁰ (1) , Ã (1) , ¢ (1) , Ë (1) , œ (1) , Å (1) , ö (1) , 就 (1) , 算 (1) , 隔 (1) , 格 (1) , 我 (1) , 都 (1) , 知 (1) \n",
      "word_counts 134\n",
      "word_docs 134\n",
      "word_index 134\n",
      "Top words\n",
      "   (1009964) , e (410864) , t (350093) , a (326856) , o (313641) , n (266951) , i (257031) , r (225972) , s (221074) , h (189357) , l (156577) , d (134529) , u (112227) , y (104642) , m (104031) , c (88177) , g (88162) , w (77364) , p (70898) , . (68748) , f (65032) , b (61793) , k (45574) , v (36469) , S (33042) , I (30214) , T (26762) , ' (25149) , , (23244) , M (22499) , A (22401) , C (21290) , B (19300) , @ (19198) , ! (16265) , D (16147) , 1 (15619) , W (15084) , N (14802) , # (14515) , R (14442) , P (14255) , F (14012) , H (13356) , O (12823) , L (12541) , G (12407) , E (12015) , 2 (11779) , : (10679) \n",
      "Last words\n",
      " + (462) , > (308) , < (293) , % (270) , = (256) , [ (205) , ] (205) , ~ (198) , ^ (143) , \\ (35) , ` (35) , ️ (27) , } (9) , – (8) , £ (7) , { (7) , \n",
      " (5) , ’ (3) , ☆ (3) , ・ (3) , — (2) , Ｏ (2) , Ｌ (2) , 你 (2) , ° (2) , İ (2) , ℃ (2) , ó (2) , ï (1) , Ｆ (1) , Ｗ (1) , ง (1) , ว (1) , ย (1) , 就 (1) , 算 (1) , 隔 (1) , 格 (1) , 我 (1) , 都 (1) , 知 (1) , 讲 (1) , « (1) , í (1) , ⁰ (1) , Ã (1) , ¢ (1) , Ë (1) , œ (1) , Å (1) \n"
     ]
    }
   ],
   "source": [
    "toks=[]\n",
    "maps_=[]\n",
    "for i,t in enumerate(tasks):\n",
    "    tweets=[t for t,c in data_train[i]]\n",
    "    tweets_=[t for t,c in data_polarity_train]\n",
    "    toks.append(buildTokenizer([normalizeText(t) for t in tweets+tweets_]))\n",
    "    maps_.append({v: k for k, v in toks[-1].word_index.items()})\n",
    "    print(\"word_counts\",len(toks[-1].word_counts))\n",
    "    print(\"word_docs\",len(toks[-1].word_docs))\n",
    "    print(\"word_index\",len(toks[-1].word_index))\n",
    "    print(\"Top words\\n\",\", \".join([\"{0} ({1}) \".format(maps_[-1][f+1],toks[i].word_counts[maps_[-1][f+1]]) for f in range(50)]))\n",
    "    print(\"Last words\\n\",\", \".join([\"{0} ({1}) \".format(maps_[-1][f],toks[i].word_counts[maps_[-1][f]]) for f in range(len(maps_[i])-50,len(maps_[i]))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example train in taskA\n",
      "The mouse's first incepted memory was just the sound : BRAAAWWWP !\n",
      "I LOVE not sleeping . It's the best .\n",
      "Religion is unfounded , else , Allah would have saved the kids . . @tariqmushtaqkh @nicpradhan #PeshawarAttack #PakSchoolSiege\n",
      "Love how I came into work at 8 because Charlie said we were busy ... 3 people in 45 minutes , yeah we got this place packed Charlie .\n",
      "Thx for catching on #urock\n",
      "\n",
      "T h e   m o u s e ' s   f i r s t   i n c e p t e d   m e m o r y   w a s   j u s t   t h e   s o u n d   :   B R A A A W W W P   !\n",
      "I   L O V E   n o t   s l e e p i n g   .   I t ' s   t h e   b e s t   .\n",
      "R e l i g i o n   i s   u n f o u n d e d   ,   e l s e   ,   A l l a h   w o u l d   h a v e   s a v e d   t h e   k i d s   .   .   @ t a r i q m u s h t a q k h   @ n i c p r a d h a n   # P e s h a w a r A t t a c k   # P a k S c h o o l S i e g e\n",
      "L o v e   h o w   I   c a m e   i n t o   w o r k   a t   8   b e c a u s e   C h a r l i e   s a i d   w e   w e r e   b u s y   . . .   3   p e o p l e   i n   4 5   m i n u t e s   ,   y e a h   w e   g o t   t h i s   p l a c e   p a c k e d   C h a r l i e   .\n",
      "T h x   f o r   c a t c h i n g   o n   # u r o c k\n",
      "\n",
      "[0, 1, 1, 1, 1]\n",
      "\n",
      "Example train in taskB\n",
      "Produce all kinds of Creative Designs #like15 | http://t.co/OXeuznMhY8 http://t.co/w4eZ9mObFJ\n",
      "there is only 1 race , HUMAN so i dont look at things by the myth of \" races \" @jtarleta53 @RBRNetwork1\n",
      "@bigbillybmoney oh haha no they ain't : face_with_tears_of_joy :: face_with_tears_of_joy : you ain't even seen em\n",
      "@Only1Neets Probably a Nigerian .\n",
      "#newjob Urgent CONTRACT - Assurance Manager - Private Banking , Singapore - Not Specified , Singapore Specifie ... http://t.co/bASQ2zWos1\n",
      "\n",
      "P r o d u c e   a l l   k i n d s   o f   C r e a t i v e   D e s i g n s   # l i k e 1 5   |   h t t p   h t t p\n",
      "t h e r e   i s   o n l y   1   r a c e   ,   H U M A N   s o   i   d o n t   l o o k   a t   t h i n g s   b y   t h e   m y t h   o f   \"   r a c e s   \"   @ j t a r l e t a 5 3   @ R B R N e t w o r k 1\n",
      "@ b i g b i l l y b m o n e y   o h   h a h a   n o   t h e y   a i n ' t   :   f a c e _ w i t h _ t e a r s _ o f _ j o y   : :   f a c e _ w i t h _ t e a r s _ o f _ j o y   :   y o u   a i n ' t   e v e n   s e e n   e m\n",
      "@ O n l y 1 N e e t s   P r o b a b l y   a   N i g e r i a n   .\n",
      "# n e w j o b   U r g e n t   C O N T R A C T   -   A s s u r a n c e   M a n a g e r   -   P r i v a t e   B a n k i n g   ,   S i n g a p o r e   -   N o t   S p e c i f i e d   ,   S i n g a p o r e   S p e c i f i e   . . .   h t t p\n",
      "\n",
      "[[1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [1, 0, 0, 0, 0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def ir2class(k):\n",
    "    onehot=[[1,0,0,0,0],[0,1,0,0,0],[0,0,1,0,0],[0,0,0,1,0],[0,0,0,0,1]]\n",
    "    return onehot[['0','1','2',\"3\",\"4\"].index(k)]\n",
    "\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "X_test=[]\n",
    "y_test=[]\n",
    "for i,t in enumerate(tasks):\n",
    "    X_train.append([text2seq(toks[i],t) for t,c in data_train[i]])\n",
    "    if t==\"taskA\":\n",
    "        y_train.append([int(c) for t,c in data_train[i]])\n",
    "    else:\n",
    "        y_train.append([ir2class(c) for t,c in data_train[i]])\n",
    "    X_test.append([text2seq(toks[i],t) for t,c in data_test[i]])\n",
    "    if t==\"taskA\":\n",
    "        y_test.append([int(c) for t,c in data_test[i]])\n",
    "    else:\n",
    "        y_test.append([ir2class(c) for t,c in data_test[i]])\n",
    "        \n",
    "\n",
    "    print(\"Example train in\",t)\n",
    "    #print(\"\\n\".join([\" \".join([str(w) for w in S]) for S in X_train[i][:5]]))\n",
    "    print(\"\\n\".join([t for t,c in data_train[i][:5]]))\n",
    "    print()\n",
    "    print(\"\\n\".join([\" \".join([maps_[i][w] for w in S if w]) for S in X_train[i][:5]]))\n",
    "    print()\n",
    "    print([c for c in y_train[i][:5]])\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example train\n",
      "F i o r i n a   b l a s t s   C l i n t o n   ,   a s k s   w h y   T r u m p   i s   M I A   -   R e p u b l i g a n   h o p e f u l   C a r l y   F i o r i n a   s a i d   M o n d a y   t h a t   s h e   f e e l s   l i k e   t   . . .   h t t p\n",
      "@ k s i z z l e N J   I ' m   d o w n   f o r   t h a t   o b v i o u s l y   ,   b u t   d o e s   N i n t e n d o   w a n t   t h a t   ?   T h e   w o r l d   m a y   n e v e r   k n o w\n",
      "@ l u g a s m c e m   T r u e   ,   r i c h t   n o w   t h e y   h a v e   t h e   b e s t   s q u a d   .   B u t   i f   M i l a n   g a n   p u l l   a   f e w   t r a n s f e r s   r i c h t   i n   J a n u a r y   ,   t h e r e ' s   h o p e   f o r   t o p   3   .\n",
      "# E M A   # M A S S I V E A C T I O N   t h i s   S u n d a y   a t   1 2   N o o n   i n   V e n i g e   B e a g h   a t   R o s e   A v e   &   O g e a n   F r o n t   W a l k   o n   t h e   s a n d   !   F r e e   . . .   h t t p\n",
      "f u l l   r e s u l t s   f r o m   E l d o r a   S p e e d w a y   !   f i n i s h e d   2 n d   w i t h   t h e   N R A   !   f b\n",
      "[[1, 0, 0], [1, 0, 0], [0, 1, 0], [1, 0, 0], [0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "X_polarity_train=[]\n",
    "y_polarity_train=[]\n",
    "for i,t in enumerate(tasks):\n",
    "    X_polarity_train.append([text2seq(toks[i],t) for t,c in data_polarity_train])\n",
    "    y_polarity_train.append([c for t,c in data_polarity_train])\n",
    "    \n",
    "print(\"Example train\")\n",
    "print(\"\\n\".join([\" \".join([maps_[i][w] for w in S if w]) for S in X_polarity_train[0][:5]]))\n",
    "print(y_polarity_train[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(task):\n",
    "    \n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    embeedings = Embedding(max_features, embedding_size)(inputs)\n",
    "    dropout=SpatialDropout1D(0.25)(embeedings)\n",
    "    \n",
    "    conv1 = Conv1D(16,\n",
    "                 3,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1)(dropout)\n",
    "    maxpool1 = MaxPooling1D(pool_size=pool_size)(conv1)\n",
    "    conv2 = Conv1D(16,\n",
    "                 5,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1)(dropout)\n",
    "    maxpool2 = MaxPooling1D(pool_size=pool_size)(conv2)\n",
    "    conv3= Conv1D(16,\n",
    "                 7,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1)(dropout)\n",
    "    maxpool3= MaxPooling1D(pool_size=pool_size)(conv3)\n",
    "    conv4= Conv1D(16,\n",
    "                 9,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1)(dropout)\n",
    "    maxpool4=MaxPooling1D(pool_size=pool_size)(conv4)\n",
    "    \n",
    "    concatenate = Maximum()([maxpool1,maxpool2,maxpool3,maxpool4])\n",
    "    bidirectional =  Bidirectional(LSTM(lstm_output_size))(concatenate)\n",
    "    if task==\"taskA\":\n",
    "        irony=Dense(1, activation='sigmoid')(bidirectional)\n",
    "    if task==\"taskB\":\n",
    "        irony=Dense(5, activation='softmax')(bidirectional)\n",
    "    \n",
    "    polarity=Dense(3, activation='softmax')(bidirectional)\n",
    "    model_irony = Model(inputs=inputs, outputs=irony)\n",
    "    model_polarity = Model(inputs=inputs, outputs=polarity)\n",
    "    return model_polarity, model_irony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model task taskA\n",
      "Creating model task taskB\n"
     ]
    }
   ],
   "source": [
    "models=[]\n",
    "for i,t in enumerate(tasks):\n",
    "    print(\"Creating model task\",t)\n",
    "    models.append(build_model(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 156)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 156, 32)      3200        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 156, 32)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 156, 16)      1552        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 156, 16)      2576        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 156, 16)      3600        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 156, 16)      4624        spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 39, 16)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 39, 16)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 39, 16)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 39, 16)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "maximum_1 (Maximum)             (None, 39, 16)       0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 64)           12544       maximum_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3)            195         bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 28,291\n",
      "Trainable params: 28,291\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 41173 samples, validate on 4575 samples\n",
      "Epoch 1/40\n",
      "17024/41173 [===========>..................] - ETA: 52s - loss: 1.0179 - acc: 0.4498"
     ]
    }
   ],
   "source": [
    "scores=[]\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "for i,t in enumerate(tasks):\n",
    "    model_pol=models[i][0]\n",
    "    model_pol.summary()\n",
    "    \n",
    "    model_pol.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    X_polarity_train_=sequence.pad_sequences(X_polarity_train[i], maxlen=maxlen)\n",
    "   \n",
    "    model_pol.fit(X_polarity_train_,y_polarity_train[i],\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs_polarity,\n",
    "                  validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i,t in enumerate(tasks):\n",
    "    print(\"Evaluating task\",t)\n",
    "    model_ir=models[i][1]\n",
    "\n",
    "    model_ir.summary()\n",
    "    \n",
    "    if t==\"taskA\":\n",
    "        model_ir.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    else:\n",
    "        model_ir.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    X_train_=sequence.pad_sequences(X_train[i], maxlen=maxlen)\n",
    "    X_test_=sequence.pad_sequences(X_test[i], maxlen=maxlen)\n",
    "    \n",
    "\n",
    "    for it in range(epochs_irony):\n",
    "        model_ir.fit(X_train_,y_train[i],\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=1,\n",
    "                  validation_split=0)\n",
    "    \n",
    "    \n",
    "        #score, acc = model.evaluate(X_test_,y_test[i],batch_size=batch_size)\n",
    "    \n",
    "        y_test_ = model_ir.predict(X_test_, batch_size=batch_size, verbose=1)\n",
    "        if t==\"taskA\":\n",
    "            y_test_ = np.round(y_test_)\n",
    "            ori = y_test[i]\n",
    "        else:\n",
    "            y_test_ = np.argmax(y_test_, axis=1)\n",
    "            ori = np.argmax(y_test[i],axis=1)\n",
    "        \n",
    "        print(classification_report(ori, y_test_))\n",
    "        print(\"Macro f-score:\", f1_score(ori, y_test_, average=\"macro\"))\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
